{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab01b412",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Neural Shape Model Tutorial #1\n",
    "\n",
    "## Learning to Fit Signed Distance Functions (SDFs) with Neural Networks\n",
    "\n",
    "This tutorial demonstrates what a signed distance field is, and how to use \n",
    "a Multi-Layer Perceptron (MLP) neural network to learn a signed distance\n",
    "function (SDF)an implicit representation of a 3D surface. We'll work with \n",
    "medical imaging data (a tibia bone) and train a network to predict \n",
    "Signed Distance Function (SDF) values for any coordinate in space. \n",
    "These are commonly called \"implicit representations\" as they implicitly, \n",
    "instead of explicitly, represent the surface. E.g., a triangle mesh\n",
    "explicitly represents the surface, whereas the function can be queried in\n",
    "a way to get the surface, but it doesn't explicitly represent it.\n",
    "\n",
    "### What is a Signed Distance Function (SDF)?\n",
    "\n",
    "An SDF is a function that, given a 3D point in space, returns:\n",
    "- **Negative values** for points inside the surface\n",
    "- **Zero** for points exactly on the surface  \n",
    "- **Positive values** for points outside the surface\n",
    "\n",
    "The absolute value represents the distance to the object surface.\n",
    "\n",
    "### Why Use Neural Networks for Implicit Surfaces?\n",
    "\n",
    "Traditional explicit representations (like meshes) have limitations:\n",
    "- Fixed topology\n",
    "- Memory scales with surface complexity\n",
    "- Difficult to edit or deform\n",
    "\n",
    "Neural implicit representations offer:\n",
    "- **Continuous surfaces** at arbitrary resolution\n",
    "- **Compact representation** (just network weights)\n",
    "- **Easy interpolation** between shapes\n",
    "- **Differentiable** for optimization\n",
    "\n",
    "Let's start by importing the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages if running in Colab\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install --upgrade pip setuptools wheel\n",
    "\n",
    "    # Download requirements.txt from the repo if not present\n",
    "    if not os.path.exists('requirements_colab.txt'):\n",
    "        !wget https://raw.githubusercontent.com/gattia/ISB-2025-Shape-Modeling/main/requirements_colab.txt\n",
    "    # Install requirements\n",
    "    !pip install -r requirements_colab.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for mesh processing and visualization\n",
    "import pymskt as mskt          # Medical imaging toolkit for mesh operations\n",
    "import glob                    # File path pattern matching\n",
    "import os                      # Operating system interface\n",
    "from itkwidgets import view    # Interactive 3D visualization in Jupyter\n",
    "import pyvista as pv           # 3D data processing and visualization\n",
    "import numpy as np             # Numerical computing\n",
    "import sys\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch libraries for deep learning\n",
    "import torch                   # Main PyTorch library\n",
    "import torch.nn as nn          # Neural network modules\n",
    "from torch.utils.data import Dataset, DataLoader  # Data handling utilities\n",
    "\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8225c2e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Loading and Visualizing the Reference Mesh\n",
    "\n",
    "We'll start by loading a 3D mesh of a tibia bone from medical imaging data. This mesh \n",
    "represents the explicit surface that we want to learn implicitly using our neural network.\n",
    "\n",
    "The mesh is stored in VTK format and contains:\n",
    "- **Vertices**: 3D points defining the surface\n",
    "- **Faces**: Triangular connectivity between vertices\n",
    "- **Surface normals**: Vectors perpendicular to the surface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf84eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1daa368ec54b35b7d587848b9ef714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(geometries=[{'vtkClass': 'vtkPolyData', 'points': {'vtkClass': 'vtkPoints', 'name': '_points', 'numberO‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the reference tibia mesh from a VTK file\n",
    "# This is a 3D triangulated surface mesh of a tibia bone from medical imaging\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Download the mesh file from GitHub if running in Colab\n",
    "    tibia_url = \"https://github.com/gattia/ISB-2025-Shape-Modeling/raw/main/data/9226874_RIGHT_tibia.vtk\"\n",
    "    local_mesh_path = \"9226874_RIGHT_tibia.vtk\"\n",
    "    if not os.path.exists(local_mesh_path):\n",
    "        !wget -O {local_mesh_path} {tibia_url}\n",
    "    path_ref_tibia = local_mesh_path\n",
    "else:\n",
    "    path_ref_tibia = './data/9226874_RIGHT_tibia.vtk'\n",
    "ref_tibia = mskt.mesh.Mesh(path_ref_tibia)\n",
    "\n",
    "# Visualize the original mesh in 3D\n",
    "# This shows the explicit surface representation we want to learn implicitly\n",
    "view(geometries=[ref_tibia])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0714411a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Data Preparation and Normalization\n",
    "\n",
    "Before training our neural network, we need to prepare high-quality training data. This step is crucial for successful SDF learning!\n",
    "\n",
    "### üéØ Our Data Pipeline\n",
    "\n",
    "1. **üîß Normalize the mesh**: Center at origin and scale for numerical stability\n",
    "2. **‚úÇÔ∏è Create 2D slice**: Simplify 3D‚Üí2D for this tutorial\n",
    "3. **üé≤ Sample training points**: Generate diverse coordinate samples\n",
    "4. **üìè Compute ground truth SDFs**: Calculate true signed distances\n",
    "5. **üìä Visualize the data**: Inspect our training examples\n",
    "\n",
    "### Why Normalize?\n",
    "\n",
    "**üî¢ Numerical Stability**: \n",
    "- Neural networks train best with inputs in consistent ranges (e.g., [-1, 1])\n",
    "- Large coordinate values can cause gradient instability and slow convergence\n",
    "- Normalization ensures all features have similar scales\n",
    "\n",
    "**üìê Scale Consistency**:\n",
    "- Makes the problem scale-invariant\n",
    "- Network learns relative distances, not absolute measurements\n",
    "- Easier to transfer knowledge between different sized objects\n",
    "\n",
    "### üéØ Smart Training Data Strategy\n",
    "\n",
    "**The Challenge**: We need point-SDF pairs to train our network, but we only have a surface mesh.\n",
    "\n",
    "**Our Solution**: Sample points around the surface using strategic noise injection:\n",
    "\n",
    "```\n",
    "1. Start with surface points ‚Üí Known SDF = 0\n",
    "2. Add small noise ‚Üí Points near surface with small |SDF|  \n",
    "3. Add larger noise ‚Üí Points far from surface with large |SDF|\n",
    "4. Add uniform samples ‚Üí Better coverage of the spatial domain\n",
    "```\n",
    "\n",
    "**üìä Three-Part Sampling Strategy:**\n",
    "\n",
    "1. **üéØ Close points** (œÉ = 0.01): Dense sampling near surface\n",
    "   - Ensures accurate surface boundary learning\n",
    "   - High density where precision matters most\n",
    "\n",
    "2. **üåç Far points** (œÉ = 0.075): Broader spatial coverage  \n",
    "   - Teaches inside/outside classification\n",
    "   - Provides context about distance gradients\n",
    "\n",
    "3. **üé≤ Uniform points**: Random spatial coverage\n",
    "   - Prevents overfitting to surface-centric distribution\n",
    "   - Improves generalization across the domain\n",
    "\n",
    "**üí° Why This Works:**\n",
    "- **Surface fidelity**: Dense near-surface samples capture fine details\n",
    "- **Spatial coverage**: Broader sampling teaches global SDF structure  \n",
    "- **Balanced learning**: Network learns both precise distances AND correct signs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1dc3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b36b1ea1bc46ba892893af264d5bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(geometries=[{'vtkClass': 'vtkPolyData', 'points': {'vtkClass': 'vtkPoints', 'name': '_points', 'numberO‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MESH NORMALIZATION: Prepare the mesh for neural network training\n",
    "# =============================================================================\n",
    "\n",
    "# Step 1: Center the mesh at origin\n",
    "mean = np.mean(ref_tibia.points, axis=0)\n",
    "ref_tibia.points -= mean\n",
    "\n",
    "# Step 2: Scale to unit norm  \n",
    "# This normalizes the scale and improves numerical stability during training\n",
    "norm = np.linalg.norm(ref_tibia.points, axis=1)  # Distance from origin for each point\n",
    "max_norm = np.max(norm)                          # Maximum distance\n",
    "ref_tibia.points /= max_norm                     # Scale all points to be within unit sphere (with buffer)\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING DATA GENERATION: Create randomly sampled points & compute SD values\n",
    "# =============================================================================\n",
    "\n",
    "# Create a 2D coronal slice (cutting through y-axis) to simplify the problem\n",
    "# This reduces the complexity from 3D to 2D for our example. \n",
    "slice_ = ref_tibia.slice('y')\n",
    "\n",
    "# Training parameters\n",
    "N = 20_000          # Number of points per distribution\n",
    "CLOSE_SD = 0.01     # Small standard deviation for points near surface\n",
    "FAR_SD = 0.075      # Larger standard deviation for points away from surface\n",
    "\n",
    "# Initialize array to hold all training points:\n",
    "# 2 distributions √ó N points each + 500 extra points to get uniformly over the space. \n",
    "pts = np.zeros((N*2 + 500, 3))\n",
    "\n",
    "# Generate two different point distributions around the surface\n",
    "for i, SD in enumerate([CLOSE_SD, FAR_SD]):\n",
    "    # Randomly sample points on the slice surface as starting locations\n",
    "    indices = (np.random.sample(N) * slice_.points.shape[0]).astype(int)\n",
    "    \n",
    "    # Generate Gaussian noise to perturb the surface points\n",
    "    # This creates points both inside (negative SDF) and outside (positive SDF) the surface\n",
    "    x_noise = np.random.normal(loc=0, scale=SD, size=N)\n",
    "    z_noise = np.random.normal(loc=0, scale=SD, size=N)\n",
    "    \n",
    "    # Add noise to the x and z coordinates (y stays at slice level ‚âà 0)\n",
    "    pts[i*N:(i+1)*N, 0] = slice_.points[indices, 0] + x_noise  # X coordinate\n",
    "    pts[i*N:(i+1)*N, 2] = slice_.points[indices, 2] + z_noise  # Z coordinate\n",
    "    # Y coordinate remains 0 (slice level)\n",
    "\n",
    "# get random uniform points over the unite cube to make the \n",
    "# network learn the whole space.\n",
    "x_uniform = (np.random.rand(500)*2 - 1)   # make points span -1, 1\n",
    "z_uniform = (np.random.rand(500)*2 - 1)   # make points span -1, 1\n",
    "\n",
    "pts[-500:, 0] = x_uniform\n",
    "pts[-500:, 2] = z_uniform\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SDF COMPUTATION: Calculate ground truth signed distances\n",
    "# =============================================================================\n",
    "\n",
    "# Convert points to PyVista format for SDF computation\n",
    "pts_ = pv.PolyData(pts)\n",
    "\n",
    "# Compute the signed distance from each point to the mesh surface\n",
    "# This gives us the ground truth SDF values that our network will learn to predict\n",
    "pts_.compute_implicit_distance(ref_tibia, inplace=True)\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION: Display the generated training data\n",
    "# =============================================================================\n",
    "\n",
    "# Visualize the slice, original mesh, and generated training points\n",
    "# Points are colored by their SDF values (blue=inside, red=outside)\n",
    "view(geometries=[slice_, ref_tibia], point_sets=pts_, point_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c128c8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Neural Network Architecture and Training\n",
    "\n",
    "Now we'll design and train a neural network to learn the implicit surface representation. This is where the magic happens ‚Äì we're teaching the network to become a continuous SDF function!\n",
    "\n",
    "### Network Architecture: Multi-Layer Perceptron (MLP)\n",
    "\n",
    "Our network is a simple but powerful feedforward neural network:\n",
    "\n",
    "```\n",
    "Input: (x, z) coordinates ‚Üí Hidden Layer 1 ‚Üí Hidden Layer 2 ‚Üí Output: SDF value\n",
    "       2D coordinates      32 neurons      32 neurons     1 scalar (distance)\n",
    "```\n",
    "\n",
    "**The network IS the SDF function**: Once trained, `model(x, z)` returns the signed distance to the surface at coordinate (x, z).\n",
    "\n",
    "### Key Design Choices:\n",
    "\n",
    "**üéØ Why These Architecture Decisions?**\n",
    "\n",
    "1. **Input dimension**: 2D (x,z) instead of 3D simplifies learning while demonstrating core concepts\n",
    "2. **Hidden size**: 32 neurons provide sufficient capacity without overfitting on our 40K samples\n",
    "3. **Activation**: ReLU creates non-linear decision boundaries essential for complex shapes\n",
    "4. **Output**: No final activation ‚Üí continuous real-valued SDF predictions\n",
    "5. **Depth**: 2 hidden layers balance expressiveness with training stability\n",
    "\n",
    "**üí° Experiment Ideas:**\n",
    "- Try different activations: `Tanh`, `Sigmoid`, `Swish` - how do they affect surface smoothness?\n",
    "- Modify architecture: More layers? Wider networks? Skip connections?\n",
    "- Add positional encoding for better high-frequency detail capture?\n",
    "\n",
    "### Training Strategy & Loss Function Design\n",
    "\n",
    "**üéØ L1 Loss (Mean Absolute Error)**\n",
    "- **Why L1?** Preserves sharp surface features better than L2/MSE\n",
    "- **L2 tends to blur** ‚Üí smooth but less accurate surfaces  \n",
    "- **L1 preserves detail** ‚Üí sharper, more accurate reconstructions\n",
    "\n",
    "**üöÄ Advanced Loss Ideas to Try:**\n",
    "- **Weighted L1**: Higher weights for points near surface (|SDF| < threshold)\n",
    "- **Sign-aware loss**: Extra penalty for wrong inside/outside classification\n",
    "- **Eikonal loss**: Regularize SDF gradients to have unit magnitude (proper distance field property)\n",
    "- **Multi-scale loss**: Train on multiple resolutions simultaneously\n",
    "\n",
    "**‚öôÔ∏è Training Configuration:**\n",
    "- **Optimizer**: Adam with adaptive learning rates (robust for coordinate networks)\n",
    "- **Batch size**: 500 points for good gradient estimates without memory issues\n",
    "- **SDF clipping**: Focus learning on surface proximity (¬±0.1 units)\n",
    "- **Data shuffling**: Prevents overfitting to specific point sequences\n",
    "\n",
    "**üìä What to Watch During Training:**\n",
    "- **Decreasing loss**: Network learning the coordinate‚ÜíSDF mapping\n",
    "- **Convergence speed**: Should reach <0.01 loss within 100-200 epochs\n",
    "- **Loss plateaus**: May indicate need for learning rate adjustment or architecture changes \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06e233fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 500 epochs...\n",
      "Training data: 40500 points\n",
      "Batch size: 500\n",
      "Network parameters: 1185\n",
      "Epoch   1/500, Loss: 0.032497\n",
      "Epoch  21/500, Loss: 0.016499\n",
      "Epoch  41/500, Loss: 0.008798\n",
      "Epoch  61/500, Loss: 0.007061\n",
      "Epoch  81/500, Loss: 0.006306\n",
      "Epoch 101/500, Loss: 0.005675\n",
      "Epoch 121/500, Loss: 0.005273\n",
      "Epoch 141/500, Loss: 0.005014\n",
      "Epoch 161/500, Loss: 0.004843\n",
      "Epoch 181/500, Loss: 0.004715\n",
      "Epoch 201/500, Loss: 0.004607\n",
      "Epoch 221/500, Loss: 0.004513\n",
      "Epoch 241/500, Loss: 0.004424\n",
      "Epoch 261/500, Loss: 0.004335\n",
      "Epoch 281/500, Loss: 0.004293\n",
      "Epoch 301/500, Loss: 0.004196\n",
      "Epoch 321/500, Loss: 0.004138\n",
      "Epoch 341/500, Loss: 0.004015\n",
      "Epoch 361/500, Loss: 0.003958\n",
      "Epoch 381/500, Loss: 0.003888\n",
      "Epoch 401/500, Loss: 0.003821\n",
      "Epoch 421/500, Loss: 0.003726\n",
      "Epoch 441/500, Loss: 0.003636\n",
      "Epoch 461/500, Loss: 0.003560\n",
      "Epoch 481/500, Loss: 0.003490\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NEURAL NETWORK ARCHITECTURE: Define the MLP for SDF prediction\n",
    "# =============================================================================\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A Multi-Layer Perceptron for learning coordinate-to-SDF mappings.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: 2D coordinates (x, z)\n",
    "    - Two hidden layers with ReLU activations\n",
    "    - Output: Single SDF value\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_dim=64, output_dim=1):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        # Define the network as a sequence of layers\n",
    "        self.net = nn.Sequential(\n",
    "            # First hidden layer: projects 2D input to hidden dimension\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),  # Non-linear activation\n",
    "            \n",
    "            # Second hidden layer: processes features\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),  # Non-linear activation\n",
    "            \n",
    "            # Output layer: produces single SDF value\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "            # No activation - we want continuous real-valued output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: coordinate -> SDF prediction\"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET CLASS: Prepare training data for PyTorch\n",
    "# =============================================================================\n",
    "\n",
    "class PointsSDFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for SDF learning.\n",
    "    \n",
    "    Features:\n",
    "    - Extracts 2D coordinates (x, z) from 3D points\n",
    "    - Clips SDF values to focus learning on surface region\n",
    "    - Handles batching and shuffling for training\n",
    "    \"\"\"\n",
    "    def __init__(self, points, sdf, max_sdf=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: numpy array of shape (N, 3) - 3D coordinates\n",
    "            sdf: numpy array of shape (N,) - ground truth SDF values\n",
    "            max_sdf: maximum SDF value to consider (clips outliers)\n",
    "        \"\"\"\n",
    "        # Extract only x and z coordinates (ignore y since we're working with a slice)\n",
    "        self.xz = torch.tensor(points[:, [0, 2]], dtype=torch.float32)\n",
    "        \n",
    "        # Convert SDF values to tensor and add batch dimension\n",
    "        self.sdf = torch.tensor(sdf, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        # Clip SDF values to focus learning on surface proximity\n",
    "        # This prevents the network from spending effort learning exact signed distance\n",
    "        # values for points far from surface... higher accuracy there doesn't help us. \n",
    "        self.sdf = torch.clamp(self.sdf, min=-max_sdf, max=max_sdf)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of training samples\"\"\"\n",
    "        return self.xz.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a single training example: (coordinates, sdf_value)\"\"\"\n",
    "        return self.xz[idx], self.sdf[idx]\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING SETUP: Initialize model, data, and optimization components\n",
    "# =============================================================================\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 500\n",
    "batch_size = 500\n",
    "network_hidden_dimension = 32\n",
    "\n",
    "# Create dataset and dataloader for efficient batch processing\n",
    "dataset = PointsSDFDataset(pts, pts_['implicit_distance'])\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size,     # Process 500 points at once for efficiency\n",
    "    shuffle=True        # Randomize order each epoch to prevent overfitting\n",
    ")\n",
    "\n",
    "# Initialize the neural network\n",
    "model = SimpleMLP(input_dim=2, hidden_dim=network_hidden_dimension, output_dim=1)\n",
    "\n",
    "# Setup optimization\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # setup optimizer... try different learning rates?\n",
    "criterion = nn.L1Loss()  # Mean Absolute Error - robust to outliers, less blurring. \n",
    "\n",
    "\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "print(f\"Training data: {len(dataset)} points\")\n",
    "print(f\"Batch size: {dataloader.batch_size}\")\n",
    "print(f\"Network parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING LOOP: Learn the coordinate-to-SDF mapping\n",
    "# =============================================================================\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Process all batches in the dataset\n",
    "    for batch_idx, (xz_batch, sdf_batch) in enumerate(dataloader):\n",
    "        # Clear gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: predict SDF values for batch of coordinates\n",
    "        sdf_pred = model(xz_batch)\n",
    "        \n",
    "        # Compute loss / error we want to optimize. \n",
    "        loss = criterion(sdf_pred, sdf_batch)\n",
    "        \n",
    "        # Backward pass: compute gradients via backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model parameters using computed gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss for epoch statistics\n",
    "        running_loss += loss.item() * xz_batch.size(0)\n",
    "    \n",
    "    # Calculate average loss for this epoch\n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    \n",
    "    # Log progress every 20 epochs\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{num_epochs}, Loss: {epoch_loss:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb2344",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Evaluating the Learned SDF\n",
    "\n",
    "Our network should now be trained to map coordinates to signed distance values. \n",
    "The decreasing loss indicates that the network is getting better at predicting \n",
    "the correct SDF values.\n",
    "\n",
    "Now let's evaluate how well our network learned the implicit surface representation:\n",
    "\n",
    "### What we'll do:\n",
    "1. **Create a dense grid** of query points across our 2D slice\n",
    "    - This could be thought of/can be used to re-create an image.\n",
    "    - However, we do not need to query on an explicit grid. We can query anywhere in space. \n",
    "2. **Predict SDF values** for each grid point using our trained network\n",
    "3. **Visualize the learned SDF field** as colored points\n",
    "4. **Extract the surface** using marching cubes on the zero level \n",
    "    set (where SD crosses from (-) to (+))\n",
    "5. **Compare** with the original surface\n",
    "\n",
    "### Understanding the Results:\n",
    "- **Negative SDF values (blue/cool colors)**: Points inside the bone\n",
    "- **Positive SDF values (red/warm colors)**: Points outside the bone  \n",
    "- **Values near zero**: Points close to the surface boundary\n",
    "- **Zero level set**: The reconstructed surface where SDF = 0\n",
    "\n",
    "The quality of our implicit representation depends on how well the network \n",
    "learned these relationships!\n",
    "\n",
    "\n",
    "### Visualization tips: \n",
    "- The default colormap is viridis (purple to yellow). \n",
    "- The colormap that is blue to red with white in the middle does well for visualizing the SDF field.\n",
    "- Look at the range of values, often it is not symmetric. It helps to make it the min/max be -0.1 to 0.1 to help visualization. \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89271237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dense grid for SDF evaluation...\n",
      "Grid size: 201 √ó 201 = 40,401 query points\n",
      "Predicting SDF values using trained network...\n",
      "SDF predictions range: [-0.2712, 0.1600]\n",
      "Points inside surface (SDF < 0): 15,903\n",
      "Points outside surface (SDF > 0): 24,498\n",
      "\n",
      "Visualizing learned SDF field...\n",
      "Color mapping: Blue = Inside surface (negative SDF), Red = Outside surface (positive SDF)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6398ae87efb47538f53b36a3ccff055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(geometries=[{'vtkClass': 'vtkPolyData', 'points': {'vtkClass': 'vtkPoints', 'name': '_points', 'numberO‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SDF FIELD VISUALIZATION: Evaluate the learned implicit representation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Generating dense grid for SDF evaluation...\")\n",
    "\n",
    "# Create a high-resolution grid of query points across our 2D slice\n",
    "GRID_RESOLUTION = 0.01  # Distance between grid points (smaller = higher resolution)\n",
    "x = np.arange(-1, 1.01, GRID_RESOLUTION)\n",
    "z = np.arange(-1, 1.01, GRID_RESOLUTION)\n",
    "\n",
    "print(f\"Grid size: {len(x)} √ó {len(z)} = {len(x) * len(z):,} query points\")\n",
    "\n",
    "# Create coordinate meshes and flatten for network input\n",
    "xx, zz = np.meshgrid(x, z)\n",
    "xz_grid = np.stack([xx.ravel(), zz.ravel()], axis=1)\n",
    "\n",
    "# Convert to PyTorch tensor for model inference\n",
    "xz_tensor = torch.from_numpy(xz_grid).float()\n",
    "\n",
    "print(\"Predicting SDF values using trained network...\")\n",
    "\n",
    "# Get SDF predictions from our trained network\n",
    "# No gradients needed since we're just evaluating (not training)\n",
    "with torch.no_grad():\n",
    "    sdf_pred = model(xz_tensor).cpu().numpy().flatten()\n",
    "\n",
    "print(f\"SDF predictions range: [{sdf_pred.min():.4f}, {sdf_pred.max():.4f}]\")\n",
    "print(f\"Points inside surface (SDF < 0): {(sdf_pred < 0).sum():,}\")\n",
    "print(f\"Points outside surface (SDF > 0): {(sdf_pred > 0).sum():,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PREPARE VISUALIZATION DATA: Convert predictions to 3D points for display\n",
    "# =============================================================================\n",
    "\n",
    "# Combine coordinates into 3D points (x, 0, z) - y=0 since we're working with a slice\n",
    "xyz_coords = np.stack([\n",
    "    xz_grid[:, 0],                    # X coordinates\n",
    "    np.zeros_like(xz_grid[:, 0]),     # Y = 0 (slice level)\n",
    "    xz_grid[:, 1]                     # Z coordinates\n",
    "], axis=1)\n",
    "\n",
    "# Create PyVista point cloud for visualization\n",
    "xyz_points = pv.PolyData(xyz_coords)\n",
    "\n",
    "# Assign predicted SDF values as scalar data for color mapping\n",
    "xyz_points['Predicted_SDF'] = sdf_pred\n",
    "\n",
    "print(\"\\nVisualizing learned SDF field...\")\n",
    "print(\"Color mapping: Blue = Inside surface (negative SDF), Red = Outside surface (positive SDF)\")\n",
    "\n",
    "# Visualize the learned SDF field alongside the original slice\n",
    "view(geometries=[slice_], point_sets=xyz_points, point_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93461b13",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Surface Reconstruction with Marching Cubes\n",
    "\n",
    "The previous visualization showed us the raw SDF field as colored points. Now we'll \n",
    "extract the actual surface by finding where the SDF crosses zero using marching cubes.\n",
    "\n",
    "### Marching Cubes Algorithm\n",
    "\n",
    "Marching cubes is a classic algorithm that:\n",
    "1. **Takes a 3D grid** of scalar values (our SDF predictions)\n",
    "2. **Finds the zero level set** (where SDF transitions from negative to positive)\n",
    "3. **Generates triangulated mesh** that represents the explicit surface\n",
    "\n",
    "### Why the Zero Level Set?\n",
    "\n",
    "Remember our SDF definition:\n",
    "- **SDF < 0**: Inside the surface  \n",
    "- **SDF = 0**: Exactly on the surface ‚Üê This is what we want!\n",
    "- **SDF > 0**: Outside the surface\n",
    "\n",
    "The zero level set gives us the reconstructed surface from our learned implicit representation.\n",
    "\n",
    "### Implementation Notes:\n",
    "\n",
    "- We create a structured 3D grid for marching cubes\n",
    "- Our slice has very small Y dimension (thickness ‚âà 0.02) \n",
    "- The algorithm will find where our network predicts SDF ‚âà 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "741e3252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5885de0bbe0649489529b0d65b4218cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(geometries=[{'vtkClass': 'vtkPolyData', 'points': {'vtkClass': 'vtkPoints', 'name': '_points', 'numberO‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the predictions of the model... \n",
    "\n",
    "# Create a grid of x and z from -1 to 1 in steps of 0.01\n",
    "x = np.arange(-1, 1.01, 0.01)\n",
    "z = np.arange(-1, 1.01, 0.01)\n",
    "xx, zz = np.meshgrid(x, z)\n",
    "xz_grid = np.stack([xx.ravel(), zz.ravel()], axis=1)\n",
    "xz_tensor = torch.from_numpy(xz_grid).float()\n",
    "\n",
    "# Get SDF predictions from the trained model\n",
    "with torch.no_grad():\n",
    "    sdf_pred = model(xz_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# combing the predictions into 3D points (x, 0, z)\n",
    "xyz = np.stack([xz_grid[:, 0], np.zeros_like(xz_grid[:, 0]), xz_grid[:, 1]], axis=1)\n",
    "\n",
    "# creeate polydata from points for visualization. \n",
    "xyz = pv.PolyData(xyz)\n",
    "\n",
    "# assign sdf to xyz for visualization.\n",
    "xyz['sdf'] = sdf_pred\n",
    "\n",
    "view(geometries=[slice_], point_sets=xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e36fa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up 3D grid for marching cubes...\n",
      "Grid dimensions: 100 √ó 2 √ó 100\n",
      "Spatial bounds: X[-1.0, 1.0], Y[-0.01, 0.01], Z[-1.0, 1.0]\n",
      "Total grid points: 20,000\n",
      "Evaluating SDF at grid points...\n",
      "Grid SDF range: [-0.2710, 0.1601]\n",
      "Running marching cubes to extract surface...\n",
      "Reconstructed mesh: 620 vertices, 620 faces\n",
      "Visualizing reconstruction results...\n",
      "\n",
      "============================================================\n",
      "RECONSTRUCTION COMPLETE!\n",
      "============================================================\n",
      "The neural network has successfully learned an implicit representation\n",
      "that can reconstruct the bone surface from any coordinate query!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b6b55e24b34c1fb6fd6d5696af54a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(geometries=[{'vtkClass': 'vtkPolyData', 'points': {'vtkClass': 'vtkPoints', 'name': '_points', 'numberO‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SURFACE RECONSTRUCTION: Extract implicit surface using marching cubes\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Setting up 3D grid for marching cubes...\")\n",
    "\n",
    "# Create a structured 3D grid for the marching cubes algorithm\n",
    "MARCHING_CUBES_RESOLUTION = 100  # Grid resolution (higher = more detail, slower)\n",
    "\n",
    "# Define the spatial bounds for our reconstruction\n",
    "x_min, y_min, z_min = -1.0, -0.01, -1.0   # Minimum coordinates\n",
    "x_max, y_max, z_max =  1.0,  0.01,  1.0    # Maximum coordinates\n",
    "\n",
    "# Calculate grid spacing based on resolution\n",
    "spacing = (\n",
    "    (x_max - x_min) / (MARCHING_CUBES_RESOLUTION - 1),  # X spacing\n",
    "    (y_max - y_min) / 1,                                # Y spacing (thin slice)\n",
    "    (z_max - z_min) / (MARCHING_CUBES_RESOLUTION - 1),  # Z spacing\n",
    ")\n",
    "\n",
    "print(f\"Grid dimensions: {MARCHING_CUBES_RESOLUTION} √ó 2 √ó {MARCHING_CUBES_RESOLUTION}\")\n",
    "print(f\"Spatial bounds: X[{x_min}, {x_max}], Y[{y_min}, {y_max}], Z[{z_min}, {z_max}]\")\n",
    "\n",
    "# Create the structured grid using PyVista\n",
    "grid = pv.ImageData(\n",
    "    dimensions=(MARCHING_CUBES_RESOLUTION, 2, MARCHING_CUBES_RESOLUTION),\n",
    "    spacing=spacing,\n",
    "    origin=(x_min, y_min, z_min),\n",
    ")\n",
    "\n",
    "# Extract grid coordinates for SDF evaluation\n",
    "x, y, z = grid.points.T\n",
    "\n",
    "print(f\"Total grid points: {len(x):,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SDF EVALUATION: Predict signed distances for the marching cubes grid\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Evaluating SDF at grid points...\")\n",
    "\n",
    "# Prepare input for our neural network: extract (x, z) coordinates\n",
    "xz_grid_3d = np.stack([x, z], axis=1)\n",
    "xz_tensor_3d = torch.from_numpy(xz_grid_3d).float()\n",
    "\n",
    "# Predict SDF values for all grid points using our trained model\n",
    "with torch.no_grad():\n",
    "    sdf_pred_3d = model(xz_tensor_3d).cpu().numpy().flatten()\n",
    "\n",
    "print(f\"Grid SDF range: [{sdf_pred_3d.min():.4f}, {sdf_pred_3d.max():.4f}]\")\n",
    "\n",
    "# =============================================================================\n",
    "# MARCHING CUBES: Extract the zero level set as a triangulated surface\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Running marching cubes to extract surface...\")\n",
    "\n",
    "# Extract the isosurface where SDF = 0 (the learned surface boundary)\n",
    "reconstructed_mesh = grid.contour([0], sdf_pred_3d, method='marching_cubes')\n",
    "\n",
    "# Compute surface normals for proper shading and visualization\n",
    "reconstructed_mesh.compute_normals(inplace=True)\n",
    "\n",
    "print(f\"Reconstructed mesh: {reconstructed_mesh.n_points:,} vertices, {reconstructed_mesh.n_cells:,} faces\")\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION: Compare reconstructed surface with original\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Visualizing reconstruction results...\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECONSTRUCTION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"The neural network has successfully learned an implicit representation\")\n",
    "print(\"that can reconstruct the bone surface from any coordinate query!\")\n",
    "\n",
    "# Display both the reconstructed surface and original slice for comparison\n",
    "view(geometries=[reconstructed_mesh, slice_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f416b4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéâ Tutorial Complete: What We Accomplished\n",
    "\n",
    "Congratulations! You've successfully implemented a neural implicit surface representation using \n",
    "signed distance functions. Here's what we achieved:\n",
    "\n",
    "### Key Accomplishments\n",
    "\n",
    "1. **üìä Learned SDF Theory**: Understood how signed distance functions represent surfaces implicitly\n",
    "2. **üî¢ Generated Training Data**: Created noise-perturbed surface samples with ground truth SDF values\n",
    "3. **üß† Built Neural Network**: Designed an MLP to map coordinates ‚Üí signed distances\n",
    "4. **üìà Trained Successfully**: Achieved low reconstruction error\n",
    "5. **üé® Visualized Results**: Saw the learned SDF field as colored point clouds\n",
    "6. **üî∫ Reconstructed Surface**: Used marching cubes to extract the zero level set\n",
    "\n",
    "### What Makes This Powerful?\n",
    "\n",
    "- **Continuous Representation**: Query SDF at any coordinate, not just discrete mesh vertices\n",
    "- **Compact Storage**: Entire surface encoded in neural network weights\n",
    "- **Differentiable**: Can optimize, interpolate, and manipulate using gradients\n",
    "- **Resolution Independent**: Reconstruct at any desired level of detail\n",
    "\n",
    "### Next Steps & Extensions\n",
    "\n",
    "Want to explore further? Try these modifications:\n",
    "\n",
    "**üéõÔ∏è Experiment with Architecture:**\n",
    "- Different activation functions (Tanh, Sigmoid, Swish)\n",
    "- More/fewer hidden layers\n",
    "- Positional encoding for high-frequency details\n",
    "\n",
    "**üìä Improve Training:**\n",
    "- Different loss functions (L2, Huber, custom SDF losses)\n",
    "- Better sampling strategies (progressive, importance sampling)\n",
    "- Regularization techniques\n",
    "\n",
    "**üåç Scale to 3D:**\n",
    "- Full 3D coordinate inputs (x, y, z)\n",
    "- Multiple bone shapes for generalization\n",
    "- Conditional networks for shape families\n",
    "\n",
    "**‚ö° Advanced Applications:**\n",
    "- Shape interpolation between different bones\n",
    "- Deformation and editing operations\n",
    "- Integration with physics simulations\n",
    "\n",
    "This tutorial provided the foundation for neural implicit \n",
    "representations ‚Äì a rapidly growing field with applications \n",
    "in computer graphics, medical imaging, and 3D AI!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc36835",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NSM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
