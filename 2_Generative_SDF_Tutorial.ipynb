{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1b8090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages if running in Colab\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # update pip and setuptools\n",
    "    !pip install --upgrade pip setuptools wheel\n",
    "    # Download requirements.txt from the repo if not present\n",
    "    if not os.path.exists('requirements_colab.txt'):\n",
    "        !wget https://raw.githubusercontent.com/gattia/ISB-2025-Shape-Modeling/main/requirements_colab.txt\n",
    "    # Install requirements\n",
    "    !pip install -r requirements_colab.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymskt as mskt\n",
    "import glob\n",
    "import os\n",
    "from itkwidgets import view\n",
    "import pyvista as pv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf84eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if running in Colab (remote) or local\n",
    "is_colab = 'google.colab' in sys.modules\n",
    "\n",
    "# Path to the JSON file listing all mesh filenames\n",
    "json_path = 'list_meshes.json'\n",
    "\n",
    "# If running in Colab, download list_meshes.json if not present\n",
    "if is_colab and not os.path.exists(json_path):\n",
    "    # Download the JSON file from the GitHub repo\n",
    "    !wget https://raw.githubusercontent.com/gattia/ISB-2025-Shape-Modeling/main/list_meshes.json -O list_meshes.json\n",
    "\n",
    "# Load the list of mesh filenames from the JSON file\n",
    "with open(json_path, 'r') as f:\n",
    "    mesh_list = json.load(f)  # mesh_list is a list of filenames\n",
    "\n",
    "list_tib_paths = []\n",
    "\n",
    "for mesh_filename in mesh_list:\n",
    "    if is_colab:\n",
    "        # Remote: load from GitHub raw content\n",
    "        base_url = \"https://raw.githubusercontent.com/gattia/ISB-2025-Shape-Modeling/main/.data\"\n",
    "        path_tib_bone = f\"{base_url}/{mesh_filename}\"\n",
    "    else:\n",
    "        # Local: load from .data directory\n",
    "        path_tib_bone = os.path.join('.data', mesh_filename)\n",
    "        # Optionally, check if file exists locally\n",
    "        if not os.path.exists(path_tib_bone):\n",
    "            continue\n",
    "    list_tib_paths.append(path_tib_bone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f632d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bone(bone, buffer=0.2):\n",
    "    mean = np.mean(bone.points, axis=0)\n",
    "    bone.points -= mean\n",
    "    norm = np.linalg.norm(bone.points, axis=1)\n",
    "    max_norm = np.max(norm)\n",
    "    bone.points /= (max_norm / (1-buffer))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "\n",
    "list_tibs = []\n",
    "\n",
    "for idx in range(n):\n",
    "    print(f'Loading: {idx}/{n}')\n",
    "    if is_colab:\n",
    "        # Remote: load from GitHub raw content\n",
    "        base_url = \"https://raw.githubusercontent.com/gattia/ISB-2025-Shape-Modeling/main/data\" # Corrected base_url\n",
    "        mesh_filename = mesh_list[idx]\n",
    "        path_tib_bone_url = f\"{base_url}/{mesh_filename}\"\n",
    "        local_path = f\"/content/{mesh_filename}\" # Local path to save the downloaded file\n",
    "        print(f\"Attempting to download: {path_tib_bone_url}\") # Print the URL\n",
    "        # Download the file\n",
    "        !wget {path_tib_bone_url} -O {local_path}\n",
    "        # Use pyvista.read to load from the local path\n",
    "        tibia_mesh = pv.read(local_path)\n",
    "    else:\n",
    "        # Local: load from .data directory\n",
    "        path_tib_bone = os.path.join('.data', mesh_list[idx])\n",
    "        # Optionally, check if file exists locally\n",
    "        if not os.path.exists(path_tib_bone):\n",
    "            continue\n",
    "        tibia_mesh = mskt.mesh.Mesh(path_tib_bone)\n",
    "\n",
    "\n",
    "    if idx == 0:\n",
    "        ref_tibia = mskt.mesh.Mesh(tibia_mesh)\n",
    "        # normalize the tibia bone\n",
    "        normalize_bone(ref_tibia)\n",
    "\n",
    "        list_tibs.append(ref_tibia)\n",
    "        print('Using first as reference... not doing registration. ')\n",
    "        continue\n",
    "\n",
    "    # load the tibia and the tibia cart\n",
    "    tibia = mskt.mesh.Mesh(tibia_mesh)\n",
    "    # normalize it pre-emptively\n",
    "    normalize_bone(tibia)\n",
    "    # register the tibia to the reference tibia\n",
    "    tibia.rigidly_register(ref_tibia, return_transformed_mesh=True)\n",
    "    list_tibs.append(tibia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1dc3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_sdf_points_on_slice(\n",
    "    mesh, \n",
    "    N=20_000, \n",
    "    close_sd=0.01, \n",
    "    far_sd=0.075, \n",
    "    slice_axis='y', \n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate noisy points on a slice of a mesh and compute their SDF values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mesh : mskt.mesh.Mesh or pyvista.PolyData\n",
    "        The (already normalized) mesh to slice and compute SDFs against.\n",
    "    N : int\n",
    "        Number of points per noise level (total points will be 2*N).\n",
    "    close_sd : float\n",
    "        Standard deviation of noise for the 'close' set.\n",
    "    far_sd : float\n",
    "        Standard deviation of noise for the 'far' set.\n",
    "    slice_axis : str\n",
    "        Axis to slice along ('x', 'y', or 'z').\n",
    "    verbose : bool\n",
    "        If True, print progress.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    slice_ : pyvista.PolyData\n",
    "        The sliced mesh.\n",
    "    pts_ : pyvista.PolyData\n",
    "        The generated points with SDF values in the 'implicit_distance' array.\n",
    "    \"\"\"\n",
    "    # Copy mesh to avoid modifying input\n",
    "    slice_ = mesh.slice(slice_axis)\n",
    "\n",
    "    pts = np.zeros((N*2, 3))\n",
    "    for i, SD in enumerate([close_sd, far_sd]):\n",
    "        if verbose:\n",
    "            print(f\"Generating points set {i} with SD={SD}\")\n",
    "        indices = (np.random.sample(N) * slice_.points.shape[0]).astype(int)\n",
    "        x = np.random.normal(loc=0, scale=SD, size=N)\n",
    "        y = np.random.normal(loc=0, scale=SD, size=N)\n",
    "        pts[i*N:(i+1)*N, 0] = slice_.points[indices, 0] + x\n",
    "        pts[i*N:(i+1)*N, 2] = slice_.points[indices, 2] + y\n",
    "\n",
    "    pts_ = pv.PolyData(pts)\n",
    "    pts_.compute_implicit_distance(mesh, inplace=True)\n",
    "    return slice_, pts_\n",
    "\n",
    "# Example usage:\n",
    "slice_, pts_ = generate_sdf_points_on_slice(\n",
    "    ref_tibia, \n",
    "    N=20_000, \n",
    "    close_sd=0.01, \n",
    "    far_sd=0.075,\n",
    "    slice_axis='y'\n",
    ")\n",
    "\n",
    "view(geometries=[slice_, ref_tibia], point_sets=pts_, point_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e233fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim=2, latent_dim=8, hidden_dim=64, output_dim=1):\n",
    "        \"\"\"\n",
    "        input_dim: dimension of input features (e.g., 2 for xz)\n",
    "        latent_dim: dimension of latent vector to concatenate\n",
    "        hidden_dim: hidden layer size\n",
    "        output_dim: output size (e.g., 1 for SDF)\n",
    "        \"\"\"\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim + latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, latent):\n",
    "        \"\"\"\n",
    "        x: (batch, input_dim)\n",
    "        latent: (batch, latent_dim) or (latent_dim,) (will be broadcasted if needed)\n",
    "        \"\"\"\n",
    "        x_cat = torch.cat([x, latent], dim=-1)\n",
    "        return self.net(x_cat)\n",
    "\n",
    "class PointsSDFDataset(Dataset):\n",
    "    def __init__(self, mesh_list, n_sample=500, N=20_000, close_sd=0.01, far_sd=0.075, max_sdf=0.1, slice_axis='y', verbose=False):\n",
    "        \"\"\"\n",
    "        mesh_list: list of meshes (already normalized) to sample from\n",
    "        N: number of points per noise level (total points per mesh will be 2*N)\n",
    "        close_sd: standard deviation for 'close' noise\n",
    "        far_sd: standard deviation for 'far' noise\n",
    "        max_sdf: clamp SDF values to [-max_sdf, max_sdf]\n",
    "        slice_axis: axis to slice along ('x', 'y', or 'z')\n",
    "        verbose: print progress\n",
    "        \"\"\"\n",
    "        self.xz = []\n",
    "        self.sdf = []\n",
    "        self.n_sample = n_sample\n",
    "        for mesh in mesh_list:\n",
    "            # Use the provided function to generate points and SDFs\n",
    "            _, pts_ = generate_sdf_points_on_slice(\n",
    "                mesh, N=N, close_sd=close_sd, far_sd=far_sd, slice_axis=slice_axis, verbose=verbose\n",
    "            )\n",
    "            pts = pts_.points  # shape (2*N, 3)\n",
    "            sdf = pts_['implicit_distance']  # shape (2*N,)\n",
    "            xz_tensor = torch.tensor(pts[:, [0, 2]], dtype=torch.float32)\n",
    "            sdf_tensor = torch.tensor(sdf, dtype=torch.float32).unsqueeze(1)\n",
    "            sdf_tensor = torch.clamp(sdf_tensor, min=-max_sdf, max=max_sdf)\n",
    "            self.xz.append(xz_tensor)\n",
    "            self.sdf.append(sdf_tensor)\n",
    "        self.point_batch_size = self.xz[0].shape[0]  # assumes all have same number of points\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of meshes\n",
    "        return len(self.xz)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Returns (idx, xz, sdf) for the idx-th mesh\n",
    "        # get n_sample random points from the xz and sdf\n",
    "        N = self.xz[idx].shape[0]\n",
    "        indices = np.random.choice(N, size=self.n_sample, replace=False)\n",
    "        return idx, self.xz[idx][indices], self.sdf[idx][indices]\n",
    "\n",
    "num_epochs = 40_000\n",
    "n_samples = 500\n",
    "batch_size = 2\n",
    "latent_dim = 32\n",
    "latent_init_std = 0.1\n",
    "slice_axis = 'y'\n",
    "\n",
    "lat_vecs = torch.nn.Embedding(len(list_tibs), latent_dim, max_norm=10.0)\n",
    "torch.nn.init.normal_(\n",
    "    lat_vecs.weight.data,\n",
    "    0.0,\n",
    "    latent_init_std / math.sqrt(latent_dim),\n",
    ")\n",
    "\n",
    "# Simple training loop for the SimpleMLP and PointsSDFDataset\n",
    "\n",
    "# Assume dataset and dataloader are already created as shown above\n",
    "# Example:\n",
    "dataset = PointsSDFDataset(\n",
    "    list_tibs, n_sample=n_samples, N=20_000,\n",
    "    close_sd=0.01, far_sd=0.075,\n",
    "    max_sdf=0.1, slice_axis=slice_axis,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = SimpleMLP(input_dim=2, latent_dim=latent_dim, hidden_dim=32, output_dim=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.L1Loss()  # Use L1 loss\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for idx, xz, sdf in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        latents = lat_vecs(idx)\n",
    "        # expand the second dimension of the latents to match the n_samples size\n",
    "        latents = latents.unsqueeze(1).expand(-1, n_samples, -1)\n",
    "        pred = model(xz, latents)\n",
    "        # pred = torch.clamp(pred, min=-0.1, max=0.1)\n",
    "        # compute an L2 norm on the latents\n",
    "        lat_loss = torch.sum(torch.norm(latents, dim=-1))\n",
    "        sdf_loss = criterion(pred, sdf)\n",
    "        loss = lat_loss + sdf_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xz.size(0)\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_IDX = 0\n",
    "\n",
    "# Create a grid of x and z from -1 to 1 in steps of 0.01\n",
    "x = np.arange(-1, 1.01, 0.01)\n",
    "z = np.arange(-1, 1.01, 0.01)\n",
    "xx, zz = np.meshgrid(x, z)\n",
    "xz_grid = np.stack([xx.ravel(), zz.ravel()], axis=1)\n",
    "xz_tensor = torch.from_numpy(xz_grid).float()\n",
    "\n",
    "# Get SDF predictions from the trained model\n",
    "with torch.no_grad():\n",
    "    latent = lat_vecs(torch.tensor(LATENT_IDX))\n",
    "    latent = latent.expand(xz_tensor.size(0), -1)\n",
    "    print(latent.size())\n",
    "    sdf_pred = model(xz_tensor, latent).cpu().numpy().flatten()\n",
    "\n",
    "# Store as xyz (x, 0, z)\n",
    "xyz = np.stack([xz_grid[:, 0], np.zeros_like(xz_grid[:, 0]), xz_grid[:, 1]], axis=1)\n",
    "\n",
    "xyz_ = pv.PolyData(xyz)\n",
    "# assign sdf to xyz_\n",
    "xyz_['sdf'] = sdf_pred\n",
    "\n",
    "view(geometries=[slice_, list_tibs[LATENT_IDX]], point_sets=xyz_, point_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d482971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a 3D SDF image using the generative model\n",
    "n = 100\n",
    "x_min, y_min, z_min = -1, -0.01, -1\n",
    "x_max, y_max, z_max = 1, 0.01, 1\n",
    "spacing = (\n",
    "    (x_max - x_min) / (n - 1),\n",
    "    0.01,\n",
    "    (z_max - z_min) / (n - 1),\n",
    ")\n",
    "grid = pv.ImageData(\n",
    "    dimensions=(n, 2, n),\n",
    "    spacing=spacing,\n",
    "    origin=(x_min, y_min, z_min),\n",
    ")\n",
    "x, y, z = grid.points.T\n",
    "\n",
    "# Prepare input for the generative model: (x, z) and latent\n",
    "xz_grid_3d = np.stack([x, z], axis=1)\n",
    "xz_tensor_3d = torch.from_numpy(xz_grid_3d).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    latent = lat_vecs(torch.tensor(LATENT_IDX))\n",
    "    latent = latent.expand(xz_tensor_3d.size(0), -1)\n",
    "    sdf_pred_3d = model(xz_tensor_3d, latent).cpu().numpy().flatten()\n",
    "\n",
    "# Marching cubes: extract the zero level set\n",
    "mesh = grid.contour([0], sdf_pred_3d, method='marching_cubes')\n",
    "mesh.compute_normals()\n",
    "\n",
    "# Visualize the mesh\n",
    "view(geometries=[mesh, slice_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f74d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a 3D SDF image using the generative model\n",
    "\n",
    "n = 100\n",
    "x_min, y_min, z_min = -1, -0.01, -1\n",
    "x_max, y_max, z_max = 1, 0.01, 1\n",
    "spacing = (\n",
    "    (x_max - x_min) / (n - 1),\n",
    "    0.01,\n",
    "    (z_max - z_min) / (n - 1),\n",
    ")\n",
    "grid = pv.ImageData(\n",
    "    dimensions=(n, 2, n),\n",
    "    spacing=spacing,\n",
    "    origin=(x_min, y_min, z_min),\n",
    ")\n",
    "x, y, z = grid.points.T\n",
    "\n",
    "# Prepare input for the generative model: (x, z) and latent\n",
    "xz_grid_3d = np.stack([x, z], axis=1)\n",
    "xz_tensor_3d = torch.from_numpy(xz_grid_3d).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    latent = lat_vecs(torch.tensor(LATENT_IDX))\n",
    "    latent = latent.expand(xz_tensor_3d.size(0), -1)\n",
    "    sdf_pred_3d = model(xz_tensor_3d, latent).cpu().numpy().flatten()\n",
    "\n",
    "# Marching cubes: extract the zero level set\n",
    "mesh = grid.contour([0], sdf_pred_3d, method='marching_cubes')\n",
    "mesh.compute_normals()\n",
    "\n",
    "slice_ = list_tibs[LATENT_IDX].slice('y')\n",
    "# slice_, pts_ = generate_sdf_points_on_slice(list_tibs[LATENT_IDX], N=20_000, close_sd=0.01, far_sd=0.075)\n",
    "\n",
    "# Visualize the mesh\n",
    "view(geometries=[mesh, slice_])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isb_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
