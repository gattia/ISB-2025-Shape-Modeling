{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "876ee20c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Neural Shape Model Tutorial #2\n",
    "\n",
    "## Generative Signed Distance Functions\n",
    "\n",
    "Building on Tutorial #1, we now explore **generative shape modeling** - training a \n",
    "single neural network that can represent multiple bone shapes and generate new variations!\n",
    "\n",
    "### üöÄ What's New in This Tutorial?\n",
    "\n",
    "**Tutorial #1** taught us to fit one neural network to **one** bone shape. \n",
    "**Tutorial #2** trains one network to represent **multiple** bone shapes using \n",
    "**latent codes**.\n",
    "\n",
    "### üß¨ Generative Shape Modeling Concepts\n",
    "\n",
    "**The Core Idea**: Instead of learning one specific shape, we learn a **shape space** - \n",
    "a continuous representation where:\n",
    "- Each point in the space represents a different bone shape\n",
    "- We can interpolate between shapes smoothly  \n",
    "- We can generate entirely new bone variations\n",
    "- All shapes share common anatomical structure\n",
    "\n",
    "### üîë Key Technical Innovations\n",
    "\n",
    "**1. Latent Embeddings**\n",
    "```\n",
    "Traditional: f(x,z) ‚Üí SDF\n",
    "Generative:  f(x,z,latent_code) ‚Üí SDF\n",
    "```\n",
    "- Each bone gets a unique **latent vector** (learnable shape code)\n",
    "- Network learns to decode coordinates + latent ‚Üí signed distance\n",
    "- Latent space captures shape variation patterns\n",
    "\n",
    "**2. Multi-Shape Training**\n",
    "- Load multiple tibia bones from different patients\n",
    "- Register them to common coordinate system\n",
    "- Train single network on all shapes simultaneously\n",
    "- Network learns shared shape patterns + individual variations\n",
    "\n",
    "**3. Shape Space Exploration**\n",
    "- Interpolate between latent codes ‚Üí morph between bone shapes\n",
    "- Generate new shapes by sampling latent space\n",
    "- Control specific shape characteristics\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you'll understand:\n",
    "1. **Latent embedding architectures** for generative modeling\n",
    "2. **Multi-shape registration** and normalization pipelines\n",
    "3. **Shared representation learning** across shape families\n",
    "4. **Shape interpolation and generation** techniques\n",
    "\n",
    "### üî¨ Applications\n",
    "\n",
    "- Statistical shape models for pathology detection\n",
    "- Patient-specific shape generation for surgical planning\n",
    "- Population-level shape analysis studies\n",
    "\n",
    "\n",
    "Let's start building our generative bone shape model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a1b8090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè† Running in local environment - assuming packages are already installed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# ENVIRONMENT SETUP: Install required packages for generative shape modeling\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"üîß Setting up Google Colab environment for generative SDF tutorial...\")\n",
    "    \n",
    "    # Update core package management tools\n",
    "    %pip install --upgrade pip setuptools wheel\n",
    "    \n",
    "    # Download requirements specific to Colab environment\n",
    "    if not os.path.exists('requirements_colab.txt'):\n",
    "        !wget https://raw.githubusercontent.com/gattia/ISB-2025-Shape-Modeling/main/requirements_colab.txt\n",
    "    \n",
    "    # Install all required packages for:\n",
    "    # - Mesh processing (pymskt, pyvista)\n",
    "    # - Neural networks (pytorch)\n",
    "    # - 3D visualization (itkwidgets)\n",
    "    # - Scientific computing (numpy, matplotlib)\n",
    "    %pip install -r requirements_colab.txt\n",
    "    \n",
    "    print(\"‚úÖ Environment setup complete!\")\n",
    "else:\n",
    "    print(\"üè† Running in local environment - assuming packages are already installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14c2b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# LIBRARY IMPORTS: Core tools for generative shape modeling\n",
    "# =============================================================================\n",
    "\n",
    "# Medical imaging and mesh processing\n",
    "import pymskt as mskt          # Advanced mesh operations and bone analysis tools\n",
    "import pyvista as pv           # 3D data processing and visualization\n",
    "from itkwidgets import view    # Interactive 3D visualization in Jupyter\n",
    "\n",
    "# Core Python libraries\n",
    "import glob                    # File path pattern matching for mesh loading\n",
    "import os                      # Operating system interface for file operations\n",
    "import json                    # JSON parsing for mesh file lists\n",
    "import sys                     # System-specific parameters and functions\n",
    "import math                    # Mathematical functions (for embedding initialization)\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np             # Numerical computing and array operations\n",
    "import matplotlib.pyplot as plt # Plotting and visualization\n",
    "\n",
    "# PyTorch deep learning framework\n",
    "import torch                   # Main PyTorch library for tensors and autograd\n",
    "import torch.nn as nn          # Neural network modules and loss functions\n",
    "from torch.utils.data import Dataset, DataLoader  # Data handling utilities for batch processing\n",
    "\n",
    "# Enable widget support for 3D visualization in Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "    print(\"üì± Enabled 3D widget support for Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEVICE CONFIGURATION: Set up GPU/CPU device for PyTorch\n",
    "# =============================================================================\n",
    "\n",
    "# Automatically detect the best available device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Or manually specify:\n",
    "# DEVICE = torch.device('cpu')        # Force CPU\n",
    "# DEVICE = torch.device('cuda')       # Force CUDA/GPU\n",
    "# DEVICE = torch.device('mps')        # Force Apple Silicon GPU\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4aedcd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Multi-Shape Data Loading\n",
    "\n",
    "### üîÑ From Single Shape to Shape Collection\n",
    "\n",
    "**Tutorial #1**: Loaded one tibia bone and learned its specific geometry  \n",
    "**Tutorial #2**: Load multiple tibia bones from different patients to learn shape variation patterns\n",
    "\n",
    "### üìä Dataset Structure\n",
    "\n",
    "Our dataset contains tibia bones from multiple patients:\n",
    "- **Left and right tibias** may exist for the same individual\n",
    "    - Left are already flipped to be rights\n",
    "- **Natural shape variation** representing population diversity\n",
    "\n",
    "### üéØ Why Multiple Shapes?\n",
    "\n",
    "**Shape Space Learning**: \n",
    "- Single shape ‚Üí Overfits to one specific geometry\n",
    "- Multiple shapes ‚Üí Learns shared anatomical patterns + individual variations\n",
    "- Network discovers what makes a \"tibia\" vs. patient-specific differences\n",
    "\n",
    "**Generative Capabilities**:\n",
    "- Can interpolate between existing patient shapes\n",
    "- Can generate new bone variations within learned distribution\n",
    "- Enables statistical shape analysis and population studies\n",
    "\n",
    "\n",
    "Let's load our bone collection and prepare for multi-shape modeling!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbf84eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Example bone files:\n",
      "   1. 9997381_RIGHT_tibia.vtk\n",
      "   2. 9226874_LEFT_tibia.vtk\n",
      "   3. 9368395_RIGHT_tibia.vtk\n",
      "   ... and 11 more bones\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MESH COLLECTION LOADING: Prepare multiple bone shapes for generative training\n",
    "# =============================================================================\n",
    "\n",
    "# Detect environment: Colab (remote) vs local development\n",
    "is_colab = 'google.colab' in sys.modules\n",
    "\n",
    "# =============================================================================\n",
    "# MANIFEST FILE LOADING: Get list of all available mesh files\n",
    "# =============================================================================\n",
    "\n",
    "json_path = 'list_meshes.json'\n",
    "\n",
    "# Download mesh manifest if running in Colab\n",
    "if is_colab and not os.path.exists(json_path):\n",
    "    print(\"üìã Downloading mesh file manifest...\")\n",
    "    !wget https://raw.githubusercontent.com/gattia/ISB-2025-Shape-Modeling/main/list_meshes.json -O list_meshes.json\n",
    "\n",
    "# Load the complete list of available mesh filenames\n",
    "# This JSON contains all tibia bone files in the dataset\n",
    "with open(json_path, 'r') as f:\n",
    "    mesh_list = json.load(f)  # List of VTK filenames\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATH PREPARATION: Build file paths for each environment\n",
    "# =============================================================================\n",
    "\n",
    "list_tib_paths = []\n",
    "\n",
    "for mesh_filename in mesh_list:\n",
    "    if is_colab:\n",
    "        # Remote: Construct URL for GitHub raw content access\n",
    "        base_url = \"https://raw.githubusercontent.com/gattia/ISB-2025-Shape-Modeling/main/data\"\n",
    "        path_tib_bone = f\"{base_url}/{mesh_filename}\"\n",
    "        \n",
    "    else:\n",
    "        # Local: Build path to local data directory\n",
    "        path_tib_bone = os.path.join('data', mesh_filename)\n",
    "        \n",
    "        # Skip if file doesn't exist locally (partial dataset)\n",
    "        if not os.path.exists(path_tib_bone):\n",
    "            print(f\"‚ö†Ô∏è Skipping missing file: {mesh_filename}\")\n",
    "            continue\n",
    "            \n",
    "    list_tib_paths.append(path_tib_bone)\n",
    "\n",
    "\n",
    "\n",
    "# Display first few filenames as examples\n",
    "print(f\"\\nüìã Example bone files:\")\n",
    "for i, filename in enumerate(mesh_list[:3]):\n",
    "    print(f\"   {i+1}. {filename}\")\n",
    "if len(mesh_list) > 3:\n",
    "    print(f\"   ... and {len(mesh_list)-3} more bones\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ef5f3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Shape Normalization and Registration\n",
    "\n",
    "### üéØ The Challenge: Aligning Multiple Bones\n",
    "\n",
    "When working with multiple bone shapes from different patients, we face several alignment challenges:\n",
    "\n",
    "- **Different positions**: Bones scanned at different locations in space\n",
    "- **Different orientations**: Varying bone rotations in the scanner\n",
    "- **Different sizes**: Natural variation in bone scale between patients\n",
    "- **Different coordinate systems**: Scanner-specific coordinate frameworks\n",
    "\n",
    "### üîß Our Normalization Strategy\n",
    " \n",
    "# **1. Centering** üìç  \n",
    " Move each bone's center of mass to the origin.\n",
    " \n",
    " **2. Scaling** üìè  \n",
    " Scale each bone to fit within a unit sphere.\n",
    " \n",
    " **3. Registration** üéØ  \n",
    " Rigidly align each bone to a reference shape.\n",
    "    - We use a similarity registration (Rigid + Scaling - 7 DOF)\n",
    "\n",
    "### üß† Why This Matters for Generative Learning\n",
    "\n",
    "**Consistent Coordinate Space**:\n",
    "- Network learns shape variation, not position/rotation differences\n",
    "- All bones occupy same spatial region ‚Üí better training efficiency\n",
    "- Latent codes capture anatomical differences, not pose differences\n",
    "\n",
    "**Numerical Stability**:\n",
    "- Coordinates in [-1, 1] range ‚Üí stable neural network training\n",
    "- Consistent scales ‚Üí gradient flow optimization  \n",
    "\n",
    "\n",
    "### ‚öñÔ∏è The Buffer Parameter\n",
    "\n",
    "Notice our normalization uses a `buffer=0.2` parameter:\n",
    "- Ensures bones don't touch the boundary of [-1,1] cube\n",
    "- Provides spatial margin for SDF computation\n",
    "- Prevents edge effects during surface reconstruction\n",
    "\n",
    "Let's implement our normalization pipeline!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f632d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SHAPE NORMALIZATION FUNCTION: Standardize bone geometry for multi-shape learning\n",
    "# =============================================================================\n",
    "\n",
    "def normalize_bone(bone, buffer=0.2):\n",
    "    \"\"\"\n",
    "    Normalize a bone mesh to a standardized coordinate system.\n",
    "    \n",
    "    This function performs spatial normalization essential for training \n",
    "    generative models on multiple bone shapes from different patients.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bone : mskt.mesh.Mesh or pyvista.PolyData\n",
    "        Input bone mesh to normalize (modified in-place)\n",
    "    buffer : float, default=0.2\n",
    "        Safety margin to keep bone away from unit cube boundaries\n",
    "        - 0.0 = bone touches [-1,1] cube faces  \n",
    "        - 0.2 = bone fits within [-0.8,0.8] cube\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Modifies the input bone mesh in-place\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Normalization steps:\n",
    "    1. Center: Translate center of mass to origin\n",
    "    2. Scale: Fit bone within unit sphere with buffer\n",
    "    \n",
    "    This ensures all bones occupy consistent spatial regions\n",
    "    while preserving relative shape proportions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # STEP 1: CENTER THE BONE AT ORIGIN\n",
    "    # Calculate center of mass (centroid) of all bone vertices\n",
    "    mean = np.mean(bone.points, axis=0)  # Shape: (3,) for [x, y, z]\n",
    "    \n",
    "    # Translate bone so its center is at (0, 0, 0)\n",
    "    bone.points -= mean\n",
    "    \n",
    "    # STEP 2: SCALE TO UNIT SPHERE WITH BUFFER\n",
    "    # Compute distance from origin for each vertex\n",
    "    norm = np.linalg.norm(bone.points, axis=1)  # Shape: (N,) distances\n",
    "    \n",
    "    # Find the furthest vertex from origin\n",
    "    max_norm = np.max(norm)\n",
    "    \n",
    "    # Scale so max distance is (1-buffer), keeping bone inside unit cube\n",
    "    # Example: buffer=0.2 ‚Üí max distance becomes 0.8\n",
    "    bone.points /= (max_norm / (1-buffer))\n",
    "    \n",
    "    # Result: Bone fits within [-0.8, 0.8]¬≥ cube when buffer=0.2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47252d74",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Multi-Shape Loading and Registration\n",
    "\n",
    "### üéØ Building Our Bone Collection\n",
    "\n",
    "Now we'll load multiple tibia bones and align them. This is where generative modeling begins to differ significantly from single-shape fitting.\n",
    "\n",
    "### üîÑ Registration Pipeline\n",
    "\n",
    "**Key Steps**:\n",
    "1. **Load first bone** ‚Üí Use as reference template  \n",
    "2. **For each subsequent bone**:\n",
    "   - Load and normalize independently\n",
    "   - Rigidly register to reference bone\n",
    "   - Add to collection\n",
    "\n",
    "### üéØ Why Registration Matters\n",
    "\n",
    "**Without Registration**:\n",
    "```\n",
    "Bone A: Pointing up    ‚Üë\n",
    "Bone B: Pointing right ‚Üí  \n",
    "Bone C: Pointing down  ‚Üì\n",
    "```\n",
    "Network learns orientation differences, not shape differences!\n",
    "\n",
    "**With Registration**:\n",
    "```\n",
    "Bone A: Pointing up    ‚Üë\n",
    "Bone B: Pointing up    ‚Üë  (rotated to match A)\n",
    "Bone C: Pointing up    ‚Üë  (rotated to match A)\n",
    "```\n",
    "Network learns shape variation!\n",
    "\n",
    "### üìä Dataset Size Considerations\n",
    "\n",
    "For this tutorial, we'll use **5 bones** to demonstrate concepts:\n",
    "- Small enough for quick training and experimentation\n",
    "- Easily scalable to larger collections\n",
    "\n",
    "Let's build our multi-shape dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "413c454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶¥ Loading and registering 10 tibia bones for generative training...\n",
      "üîÑ Processing bone 1/10\n",
      "üîÑ Processing bone 2/10\n",
      "WARNING: Mesh is now synonymous with pyvista.PolyData and thus this property is redundant and the Mesh object can be used for anything that pyvista.PolyData or vtk.vtkPolyData can be used for.\n",
      "üîÑ Processing bone 3/10\n",
      "WARNING: Mesh is now synonymous with pyvista.PolyData and thus this property is redundant and the Mesh object can be used for anything that pyvista.PolyData or vtk.vtkPolyData can be used for.\n",
      "üîÑ Processing bone 4/10\n",
      "WARNING: Mesh is now synonymous with pyvista.PolyData and thus this property is redundant and the Mesh object can be used for anything that pyvista.PolyData or vtk.vtkPolyData can be used for.\n",
      "üîÑ Processing bone 5/10\n",
      "WARNING: Mesh is now synonymous with pyvista.PolyData and thus this property is redundant and the Mesh object can be used for anything that pyvista.PolyData or vtk.vtkPolyData can be used for.\n",
      "üîÑ Processing bone 6/10\n",
      "WARNING: Mesh is now synonymous with pyvista.PolyData and thus this property is redundant and the Mesh object can be used for anything that pyvista.PolyData or vtk.vtkPolyData can be used for.\n",
      "üîÑ Processing bone 7/10\n",
      "WARNING: Mesh is now synonymous with pyvista.PolyData and thus this property is redundant and the Mesh object can be used for anything that pyvista.PolyData or vtk.vtkPolyData can be used for.\n",
      "üîÑ Processing bone 8/10\n",
      "WARNING: Mesh is now synonymous with pyvista.PolyData and thus this property is redundant and the Mesh object can be used for anything that pyvista.PolyData or vtk.vtkPolyData can be used for.\n",
      "üîÑ Processing bone 9/10\n",
      "WARNING: Mesh is now synonymous with pyvista.PolyData and thus this property is redundant and the Mesh object can be used for anything that pyvista.PolyData or vtk.vtkPolyData can be used for.\n",
      "üîÑ Processing bone 10/10\n",
      "WARNING: Mesh is now synonymous with pyvista.PolyData and thus this property is redundant and the Mesh object can be used for anything that pyvista.PolyData or vtk.vtkPolyData can be used for.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MULTI-SHAPE LOADING AND REGISTRATION: Build aligned bone collection\n",
    "# =============================================================================\n",
    "\n",
    "# Number of bones to load for generative modeling\n",
    "n = 10\n",
    "print(f\"ü¶¥ Loading and registering {n} tibia bones for generative training...\")\n",
    "\n",
    "# Initialize collection to store all registered bones\n",
    "list_tibs = []\n",
    "\n",
    "# =============================================================================\n",
    "# ITERATIVE LOADING: Process each bone with registration to reference\n",
    "# =============================================================================\n",
    "\n",
    "for idx in range(n):\n",
    "    print(f'üîÑ Processing bone {idx+1}/{n}')\n",
    "    \n",
    "    # Handle different loading strategies for Colab vs local environments\n",
    "    if is_colab:\n",
    "        # COLAB: Download bone file from GitHub and load locally\n",
    "        base_url = \"https://raw.githubusercontent.com/gattia/ISB-2025-Shape-Modeling/main/data\"\n",
    "        mesh_filename = mesh_list[idx]\n",
    "        path_tib_bone_url = f\"{base_url}/{mesh_filename}\"\n",
    "        local_path = f\"/content/{mesh_filename}\"  # Temporary local storage\n",
    "                \n",
    "        # Download bone file to Colab's temporary storage\n",
    "        !wget {path_tib_bone_url} -O {local_path} -q\n",
    "        \n",
    "        # Load mesh using PyVista (simpler for downloaded files)\n",
    "        tibia_mesh = pv.read(local_path)\n",
    "        \n",
    "    else:\n",
    "        # LOCAL: Load directly from local data directory\n",
    "        path_tib_bone = os.path.join('data', mesh_list[idx])\n",
    "        \n",
    "        # Skip if file doesn't exist in local dataset\n",
    "        if not os.path.exists(path_tib_bone):\n",
    "            print(f\"‚ö†Ô∏è File not found locally: {path_tib_bone}\")\n",
    "            continue\n",
    "            \n",
    "        # Load using pymskt for advanced mesh operations\n",
    "        tibia_mesh = mskt.mesh.Mesh(path_tib_bone)\n",
    "\n",
    "    # =============================================================================\n",
    "    # REFERENCE BONE SETUP: First bone becomes template for registration\n",
    "    # =============================================================================\n",
    "    \n",
    "    if idx == 0:\n",
    "        \n",
    "        # Convert to pymskt.Mesh for advanced operations\n",
    "        ref_tibia = mskt.mesh.Mesh(tibia_mesh)\n",
    "        \n",
    "        # Normalize to standard coordinate system\n",
    "        normalize_bone(ref_tibia)\n",
    "        \n",
    "        # Add to collection (no registration needed for reference)\n",
    "        list_tibs.append(ref_tibia)\n",
    "        continue\n",
    "\n",
    "    # =============================================================================\n",
    "    # SUBSEQUENT BONES: Normalize and register to reference template\n",
    "    # =============================================================================\n",
    "        \n",
    "    # Convert to pymskt.Mesh for registration capabilities\n",
    "    tibia = mskt.mesh.Mesh(tibia_mesh)\n",
    "    \n",
    "    # Apply same normalization as reference bone\n",
    "    normalize_bone(tibia)\n",
    "    \n",
    "    # Perform rigid registration to align with reference bone\n",
    "    # This finds the best rotation + translation to match reference shape\n",
    "    tibia.rigidly_register(ref_tibia, return_transformed_mesh=True)\n",
    "    \n",
    "    # Add registered bone to collection\n",
    "    list_tibs.append(tibia)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ea09c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Multi-Shape SDF Data Generation\n",
    "\n",
    "### üîÑ From Single-Shape to Multi-Shape Training Data\n",
    "\n",
    "**Tutorial #1**: Generated noisy points around one bone slice  \n",
    "**Tutorial #2**: Generate noisy points around multiple bone slices\n",
    "\n",
    "### üéØ Generative Training Data Strategy\n",
    "\n",
    "For each bone in our collection, we need to:\n",
    "1. **Create 2D slice** through the registered bone\n",
    "2. **Sample training points** around the slice boundary  \n",
    "3. **Compute ground truth SDFs** for each point\n",
    "\n",
    "### üß† Why Multi-Shape Data Generation?\n",
    "\n",
    "**Shared Spatial Learning**:\n",
    "- All bones occupy same coordinate region after registration\n",
    "- Network learns individual bone differences captured via latent codes\n",
    "\n",
    "**Balanced Training**:\n",
    "- Equal sampling from each bone prevents bias toward any shape\n",
    "- Network sees diverse shape variations in each batch\n",
    "- Better generalization to unseen bone variations\n",
    "\n",
    "**Regularized Latent**:\n",
    "- The latent embeddings are penalized with an L2 norm\n",
    "    - This promotes a continuous/compact latent space\n",
    "    - Results in roughly diagonal covariance matrix\n",
    "\n",
    "### üìä Training Data Structure\n",
    "\n",
    "```python\n",
    "For each bone_i in collection:\n",
    "    slice_i = bone_i.slice('y', origin=(0,0,0))\n",
    "    points_i, sdf_i = generate_sdf_points(slice_i)\n",
    "    \n",
    "Training batch = {\n",
    "    'coordinates': [points_1, points_2, ..., points_n],\n",
    "    'sdf_values': [sdf_1, sdf_2, ..., sdf_n], \n",
    "    'bone_ids': [0, 1, ..., n-1],  # Which bone each sample came from\n",
    "    'latent_codes': [z_0, z_1, ..., z_{n-1}]  # Learnable shape embeddings\n",
    "}\n",
    "```\n",
    "\n",
    "### üéõÔ∏è Data Generation Parameters\n",
    "\n",
    "Our function maintains the same sampling strategy as Tutorial #1:\n",
    "- **Close noise** (œÉ=0.01): High-precision surface boundary learning\n",
    "- **Far noise** (œÉ=0.075): Inside/outside classification and distance gradients\n",
    "- **Point count**: 20K points per bone for sufficient sampling density\n",
    "\n",
    "### üí´ The Key Innovation: Shape-Aware Sampling\n",
    "\n",
    "Unlike Tutorial #1 where we had global points and SDF values, now we have:\n",
    "- **Shape-specific points**: Each point knows which bone it came from\n",
    "- **Learnable latent codes**: Network learns unique embedding for each bone\n",
    "- **Shared coordinate space**: All points use same spatial coordinate system\n",
    "\n",
    "This enables the network to learn: `f(x, z, latent_bone_id) ‚Üí SDF`\n",
    "\n",
    "Let's implement our multi-shape data generation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff1dc3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e0df6178364587b827370c279cdf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(geometries=[{'vtkClass': 'vtkPolyData', 'points': {'vtkClass': 'vtkPoints', 'name': '_points', 'numberO‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# SDF POINT GENERATION FUNCTION: Create training data for generative modeling\n",
    "# =============================================================================\n",
    "\n",
    "def generate_sdf_points_on_slice(\n",
    "    mesh, \n",
    "    N=20_000, \n",
    "    close_sd=0.01, \n",
    "    far_sd=0.075, \n",
    "    slice_axis='y', \n",
    "    verbose=True,\n",
    "    n_uniform=500\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate noisy training points around a bone slice with ground truth SDF values.\n",
    "    \n",
    "    This function creates the training data needed for generative SDF learning\n",
    "    by sampling points around bone surfaces and computing their signed distances.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mesh : mskt.mesh.Mesh or pyvista.PolyData\n",
    "        The (already normalized and registered) bone mesh to process\n",
    "    N : int, default=20_000\n",
    "        Number of points per noise distribution (total = 2*N points)\n",
    "    close_sd : float, default=0.01\n",
    "        Standard deviation for near-surface point sampling\n",
    "        - Smaller values ‚Üí points closer to surface ‚Üí better boundary precision\n",
    "    far_sd : float, default=0.075  \n",
    "        Standard deviation for far-surface point sampling\n",
    "        - Larger values ‚Üí points further from surface ‚Üí better inside/outside learning\n",
    "    slice_axis : str, default='y'\n",
    "        Axis to slice along ('x', 'y', or 'z') - reduces 3D problem to 2D\n",
    "    verbose : bool, default=True\n",
    "        Whether to print progress messages during generation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    slice_ : pyvista.PolyData\n",
    "        The 2D slice extracted from the input mesh\n",
    "    pts_ : pyvista.PolyData\n",
    "        Generated training points with ground truth SDF values stored in \n",
    "        the 'implicit_distance' array\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Training point generation strategy:\n",
    "    1. Create 2D slice through bone (reduces computational complexity)\n",
    "    2. Sample N points with small noise ‚Üí near-surface training data\n",
    "    3. Sample N points with large noise ‚Üí far-surface training data  \n",
    "    4. Compute exact SDF values for all points using mesh geometry\n",
    "    \n",
    "    This provides balanced training data for both surface precision\n",
    "    and spatial understanding across the coordinate domain.\n",
    "    \"\"\"\n",
    "    \n",
    "    # =============================================================================\n",
    "    # SLICE EXTRACTION: Reduce 3D bone to 2D cross-section\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Create 2D slice through the bone at y‚âà0 (after registration)\n",
    "    # This simplifies the learning problem while maintaining anatomical structure\n",
    "    slice_ = mesh.slice(slice_axis, origin=(0, 0, 0))\n",
    "\n",
    "    # =============================================================================\n",
    "    # POINT SAMPLING: Generate diverse training coordinates\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Initialize array for all training points (2 distributions √ó N points each)\n",
    "    pts = np.zeros((N*2 + n_uniform, 3))\n",
    "    \n",
    "    # Generate two different noise distributions for comprehensive training\n",
    "    for i, SD in enumerate([close_sd, far_sd]):\n",
    "        distribution_name = \"near-surface\" if i == 0 else \"far-surface\"\n",
    "        \n",
    "        # STEP 1: Randomly select seed points on the slice boundary\n",
    "        # These serve as starting locations for noise perturbation\n",
    "        indices = (np.random.sample(N) * slice_.points.shape[0]).astype(int)\n",
    "        \n",
    "        # STEP 2: Generate Gaussian noise for coordinate perturbation\n",
    "        # This creates points both inside (negative SDF) and outside (positive SDF)\n",
    "        x_noise = np.random.normal(loc=0, scale=SD, size=N)\n",
    "        z_noise = np.random.normal(loc=0, scale=SD, size=N)\n",
    "        \n",
    "        # STEP 3: Apply noise to create diverse point distribution\n",
    "        pts[i*N:(i+1)*N, 0] = slice_.points[indices, 0] + x_noise  # X coordinate\n",
    "        pts[i*N:(i+1)*N, 2] = slice_.points[indices, 2] + z_noise  # Z coordinate\n",
    "        # Y coordinate stays 0 (slice level) - will be handled by network input\n",
    "    \n",
    "    # STEP 4: Sample uniformly distributed points\n",
    "    # This ensures we have a uniform distribution of points across the slice\n",
    "    # get points uniformly from -1 to 1 for x and z\n",
    "    pts[-n_uniform:, 0] = np.random.uniform(-1, 1, n_uniform)\n",
    "    pts[-n_uniform:, 2] = np.random.uniform(-1, 1, n_uniform)\n",
    "    # Y coordinate stays 0 (slice level) - will be handled by network input\n",
    "\n",
    "    # =============================================================================\n",
    "    # SDF COMPUTATION: Calculate ground truth signed distances\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Convert points to PyVista format for SDF computation\n",
    "    pts_ = pv.PolyData(pts)\n",
    "    \n",
    "    \n",
    "    # Compute signed distance from each point to the original mesh surface\n",
    "    # This gives us the target values our neural network will learn to predict\n",
    "    pts_.compute_implicit_distance(mesh, inplace=True)\n",
    "    \n",
    "    return slice_, pts_\n",
    "\n",
    "# =============================================================================\n",
    "# DEMONSTRATION: Generate training data for reference bone\n",
    "# =============================================================================\n",
    "# Generate example training data using the reference bone\n",
    "slice_, pts_ = generate_sdf_points_on_slice(\n",
    "    ref_tibia, \n",
    "    N=20_000, \n",
    "    close_sd=0.01, \n",
    "    far_sd=0.075,\n",
    "    slice_axis='y'\n",
    ")\n",
    "\n",
    "# Visualize the slice, full bone, and generated training points\n",
    "view(geometries=[slice_, ref_tibia], point_sets=pts_, point_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0684b076",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Generative Neural Network Architecture\n",
    "\n",
    "### üöÄ From Single-Shape to Multi-Shape Networks\n",
    "\n",
    "**Tutorial #1 Architecture**:\n",
    "```\n",
    "f(x, z) ‚Üí SDF\n",
    "Input: 2D coordinates\n",
    "Output: Signed distance for ONE specific bone\n",
    "```\n",
    "\n",
    "**Tutorial #2 Architecture**:\n",
    "```\n",
    "f(x, z, latent_code) ‚Üí SDF  \n",
    "Input: 2D coordinates + shape embedding\n",
    "Output: Signed distance for ANY bone in our collection\n",
    "```\n",
    "\n",
    "### üß¨ Key Innovation: Latent Shape Embeddings\n",
    "\n",
    "**The Challenge**: How can one network represent multiple different bones?\n",
    "\n",
    "**The Solution**: **Latent Codes** - learnable vector embeddings that encode shape identity\n",
    "\n",
    "```python\n",
    "# Each bone gets a unique learnable vector\n",
    "bone_0: [0.1, -0.3, 0.7, 0.2, ...]  # 32-dimensional embedding\n",
    "bone_1: [0.4, 0.1, -0.2, 0.8, ...]  # Different embedding\n",
    "bone_2: [-0.2, 0.6, 0.3, -0.1, ...]  # Yet another embedding\n",
    "\n",
    "# Network learns: coordinates + bone_embedding ‚Üí SDF\n",
    "network(x=0.5, z=0.3, latent=bone_0_embedding) ‚Üí SDF for bone 0\n",
    "network(x=0.5, z=0.3, latent=bone_1_embedding) ‚Üí SDF for bone 1\n",
    "```\n",
    "\n",
    "### üéØ Architecture Components\n",
    "\n",
    "**1. Latent Embedding Layer**\n",
    "- `torch.nn.Embedding(num_bones, latent_dim)`\n",
    "- Creates learnable vector for each bone in our collection\n",
    "- Vectors start random, optimize during training to compactly capture shape differences\n",
    "\n",
    "**2. Enhanced MLP Architecture**\n",
    "```\n",
    "Input: [x, z, latent_vector] ‚Üí Concatenated features\n",
    "Hidden Layer 1: 64 neurons + ReLU\n",
    "Hidden Layer 2: 64 neurons + ReLU  \n",
    "Output: Single SDF value\n",
    "```\n",
    "\n",
    "**3. Multi-Shape Dataset**\n",
    "- Samples points from ALL bones during training\n",
    "- Each sample includes coordinate + bone_id\n",
    "- Network learns shared patterns + individual variations\n",
    "\n",
    "### üß† How Learning Works\n",
    "\n",
    "**During Training**:\n",
    "1. **Sample batch**: Mix of points from different bones\n",
    "2. **Lookup embedding**: Get latent vector for each bone_id\n",
    "3. **Concatenate features**: [x, z, latent_vector] \n",
    "4. **Predict SDF**: Network outputs signed distance\n",
    "5. **Update weights**: Both MLP weights AND latent embeddings improve\n",
    "\n",
    "### üé® Generative Capabilities\n",
    "\n",
    "Once trained, we can:\n",
    "\n",
    "**1. Reconstruct Known Bones**\n",
    "```python\n",
    "bone_id = 2\n",
    "latent = embedding_layer(bone_id)\n",
    "sdf = network(coordinates, latent)  # Recreate bone 2\n",
    "```\n",
    "\n",
    "**2. Interpolate Between Bones**  \n",
    "```python\n",
    "latent_A = embedding_layer(bone_A)\n",
    "latent_B = embedding_layer(bone_B)\n",
    "latent_interp = 0.7 * latent_A + 0.3 * latent_B  # 70% A, 30% B\n",
    "sdf = network(coordinates, latent_interp)  # New hybrid bone!\n",
    "```\n",
    "\n",
    "**3. Generate Novel Bones**\n",
    "```python\n",
    "latent_new = sample_from_latent_distribution()  # Random vector\n",
    "sdf = network(coordinates, latent_new)  # Entirely new bone variation\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Training Configuration\n",
    "\n",
    "**Key Parameters**:\n",
    "- `latent_dim = 32`: Size of shape embedding vectors\n",
    "- `batch_size = 2`: Number of bones per training batch  \n",
    "- `n_samples = 500`: Points sampled per bone per batch\n",
    "- `epochs = 40,000`: Extended training for generative learning\n",
    "\n",
    "**Loss Function**:\n",
    "```python\n",
    "loss = L1_loss(predicted_sdf, true_sdf) + regularization(latent_embeddings)\n",
    "```\n",
    "\n",
    "**Why Regularization?**: Encourages smooth shape space - can interpolate between shapes. \n",
    "\n",
    "Let's build our generative shape model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e233fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuring generative SDF training...\n",
      "\n",
      "üöÄ Starting generative SDF training...\n",
      "Epoch     1/40,000 | Loss: 99.810660\n",
      "Epoch  1001/40,000 | Loss: 99.629613\n",
      "Epoch  2001/40,000 | Loss: 99.625876\n",
      "Epoch  3001/40,000 | Loss: 99.625165\n",
      "Epoch  4001/40,000 | Loss: 99.624567\n",
      "Epoch  5001/40,000 | Loss: 99.623991\n",
      "Epoch  6001/40,000 | Loss: 99.623381\n",
      "Epoch  7001/40,000 | Loss: 99.623387\n",
      "Epoch  8001/40,000 | Loss: 99.622818\n",
      "Epoch  9001/40,000 | Loss: 99.622792\n",
      "Epoch 10001/40,000 | Loss: 99.622437\n",
      "Epoch 11001/40,000 | Loss: 99.622279\n",
      "Epoch 12001/40,000 | Loss: 99.622377\n",
      "Epoch 13001/40,000 | Loss: 99.622137\n",
      "Epoch 14001/40,000 | Loss: 99.622057\n",
      "Epoch 15001/40,000 | Loss: 99.621951\n",
      "Epoch 16001/40,000 | Loss: 99.621799\n",
      "Epoch 17001/40,000 | Loss: 99.621756\n",
      "Epoch 18001/40,000 | Loss: 99.621898\n",
      "Epoch 19001/40,000 | Loss: 99.621808\n",
      "Epoch 20001/40,000 | Loss: 99.621698\n",
      "Epoch 21001/40,000 | Loss: 99.621571\n",
      "Epoch 22001/40,000 | Loss: 99.621632\n",
      "Epoch 23001/40,000 | Loss: 99.621707\n",
      "Epoch 24001/40,000 | Loss: 99.621541\n",
      "Epoch 25001/40,000 | Loss: 99.621508\n",
      "Epoch 26001/40,000 | Loss: 99.621378\n",
      "Epoch 27001/40,000 | Loss: 99.621429\n",
      "Epoch 28001/40,000 | Loss: 99.621373\n",
      "Epoch 29001/40,000 | Loss: 99.621280\n",
      "Epoch 30001/40,000 | Loss: 99.621405\n",
      "Epoch 31001/40,000 | Loss: 99.621204\n",
      "Epoch 32001/40,000 | Loss: 99.621159\n",
      "Epoch 33001/40,000 | Loss: 99.621249\n",
      "Epoch 34001/40,000 | Loss: 99.621323\n",
      "Epoch 35001/40,000 | Loss: 99.621237\n",
      "Epoch 36001/40,000 | Loss: 99.621109\n",
      "Epoch 37001/40,000 | Loss: 99.621138\n",
      "Epoch 38001/40,000 | Loss: 99.621170\n",
      "Epoch 39001/40,000 | Loss: 99.621184\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# GENERATIVE NEURAL NETWORK ARCHITECTURE: Multi-shape SDF learning\n",
    "# =============================================================================\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced Multi-Layer Perceptron for generative SDF learning.\n",
    "    \n",
    "    Key difference from Tutorial #1: This network accepts both spatial coordinates\n",
    "    AND latent shape embeddings, enabling it to represent multiple bone shapes.\n",
    "    \n",
    "    Architecture: [coordinates + latent] ‚Üí hidden ‚Üí hidden ‚Üí SDF\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2, latent_dim=32, hidden_dim=64, output_dim=1):\n",
    "        \"\"\"\n",
    "        Initialize generative SDF network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int, default=2\n",
    "            Spatial coordinate dimensions (x, z for 2D slice)\n",
    "        latent_dim : int, default=32\n",
    "            Size of latent shape embedding vectors\n",
    "        hidden_dim : int, default=64  \n",
    "            Number of neurons in hidden layers\n",
    "        output_dim : int, default=1\n",
    "            Output dimensions (1 for signed distance)\n",
    "        \"\"\"\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Network concatenates spatial coordinates with shape embedding\n",
    "        total_input_dim = input_dim + latent_dim\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            # First hidden layer: Process combined spatial + shape features\n",
    "            nn.Linear(total_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Second hidden layer: Refine feature representations\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Output layer: Single SDF prediction\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, latent):\n",
    "        \"\"\"\n",
    "        Forward pass: Predict SDF from coordinates and shape embedding.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor, shape (batch_size, input_dim)\n",
    "            Spatial coordinates (e.g., x, z positions)\n",
    "        latent : torch.Tensor, shape (batch_size, latent_dim)\n",
    "            Shape embedding vectors for each sample\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor, shape (batch_size, 1)\n",
    "            Predicted signed distance values\n",
    "        \"\"\"\n",
    "        # Concatenate spatial coordinates with shape embeddings\n",
    "        x_cat = torch.cat([x, latent], dim=-1)\n",
    "        \n",
    "        # Process through MLP to predict SDF\n",
    "        return self.net(x_cat)\n",
    "\n",
    "# =============================================================================\n",
    "# MULTI-SHAPE DATASET: Training data for generative learning\n",
    "# =============================================================================\n",
    "\n",
    "class PointsSDFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for multi-shape generative SDF training.\n",
    "    \n",
    "    Key innovation: Generates training data from multiple bone shapes\n",
    "    and associates each sample with its source bone identity.\n",
    "    \"\"\"\n",
    "    def __init__(self, mesh_list, n_sample=500, N=20_000, close_sd=0.01, \n",
    "                 far_sd=0.075, max_sdf=0.1, slice_axis='y', verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize multi-shape training dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mesh_list : list of mskt.mesh.Mesh\n",
    "            Collection of registered bone meshes\n",
    "        n_sample : int, default=500\n",
    "            Number of points to sample per bone per training batch\n",
    "        N : int, default=20_000\n",
    "            Total points generated per bone (2*N due to two noise levels)\n",
    "        close_sd : float, default=0.01\n",
    "            Standard deviation for near-surface point sampling\n",
    "        far_sd : float, default=0.075\n",
    "            Standard deviation for far-surface point sampling  \n",
    "        max_sdf : float, default=0.1\n",
    "            Clamp SDF values to focus learning on surface proximity\n",
    "        slice_axis : str, default='y'\n",
    "            Axis for 2D slice extraction\n",
    "        verbose : bool, default=False\n",
    "            Print progress during data generation\n",
    "        \"\"\"\n",
    "        \n",
    "        self.xz = []        # Spatial coordinates for each bone\n",
    "        self.sdf = []       # SDF values for each bone  \n",
    "        self.n_sample = n_sample\n",
    "        \n",
    "        # Generate training data for each bone in the collection\n",
    "        for i, mesh in enumerate(mesh_list):\n",
    "            if verbose:\n",
    "                print(f\"   Processing bone {i+1}/{len(mesh_list)}\")\n",
    "                \n",
    "            # Generate SDF training points using our established function\n",
    "            _, pts_ = generate_sdf_points_on_slice(\n",
    "                mesh, N=N, close_sd=close_sd, far_sd=far_sd, \n",
    "                slice_axis=slice_axis, verbose=verbose\n",
    "            )\n",
    "            \n",
    "            # Extract coordinate data (x, z) - ignore y since we're using slices\n",
    "            pts = pts_.points  # Shape: (2*N, 3)\n",
    "            sdf = pts_['implicit_distance']  # Shape: (2*N,)\n",
    "            \n",
    "            # Convert to PyTorch tensors\n",
    "            xz_tensor = torch.tensor(pts[:, [0, 2]], dtype=torch.float32).to(DEVICE)\n",
    "            sdf_tensor = torch.tensor(sdf, dtype=torch.float32).unsqueeze(1).to(DEVICE)\n",
    "            \n",
    "            # Clamp SDF values to focus learning on surface region\n",
    "            sdf_tensor = torch.clamp(sdf_tensor, min=-max_sdf, max=max_sdf)\n",
    "            \n",
    "            # Store bone-specific training data\n",
    "            self.xz.append(xz_tensor)\n",
    "            self.sdf.append(sdf_tensor)\n",
    "        \n",
    "        # Verify consistent data sizes across all bones\n",
    "        self.point_batch_size = self.xz[0].shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of bones in dataset.\"\"\"\n",
    "        return len(self.xz)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Sample training data for the idx-th bone.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (bone_idx, coordinates, sdf_values)\n",
    "            - bone_idx: Which bone this data came from (for latent lookup)\n",
    "            - coordinates: Random sample of spatial positions  \n",
    "            - sdf_values: Corresponding ground truth signed distances\n",
    "        \"\"\"\n",
    "        # Randomly sample n_sample points from this bone's full dataset\n",
    "        N = self.xz[idx].shape[0]\n",
    "        indices = np.random.choice(N, size=self.n_sample, replace=False)\n",
    "        \n",
    "        return idx, self.xz[idx][indices], self.sdf[idx][indices]\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION: Setup for generative learning\n",
    "# =============================================================================\n",
    "\n",
    "print(\"‚öôÔ∏è Configuring generative SDF training...\")\n",
    "\n",
    "# Training hyperparameters - optimized for multi-shape learning\n",
    "num_epochs = 40_000      # Extended training for generative models\n",
    "n_samples = 500          # Points sampled per bone per batch\n",
    "batch_size = 2           # Number of different bones per training batch\n",
    "latent_dim = 8          # Dimensionality of shape embeddings\n",
    "latent_init_std = 0.1    # Initial scale of latent vectors\n",
    "slice_axis = 'y'         # Slice direction for 2D learning\n",
    "\n",
    "# =============================================================================\n",
    "# LATENT EMBEDDING LAYER: Learnable shape codes\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Create embedding layer: each bone gets a learnable latent vector\n",
    "lat_vecs = torch.nn.Embedding(\n",
    "    num_embeddings=len(list_tibs),  # One embedding per bone\n",
    "    embedding_dim=latent_dim,       # 32-dimensional shape codes\n",
    "    max_norm=10.0                   # Prevent embeddings from growing too large\n",
    ").to(DEVICE)\n",
    "\n",
    "# Initialize embeddings with small random values\n",
    "# This ensures training starts from a reasonable state\n",
    "torch.nn.init.normal_(\n",
    "    lat_vecs.weight.data,\n",
    "    mean=0.0,\n",
    "    std=latent_init_std / math.sqrt(latent_dim),  # Scaled initialization\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET AND MODEL INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Create multi-shape dataset\n",
    "dataset = PointsSDFDataset(\n",
    "    mesh_list=list_tibs, \n",
    "    n_sample=n_samples, \n",
    "    N=20_000,\n",
    "    close_sd=0.01, \n",
    "    far_sd=0.075,\n",
    "    max_sdf=0.1, \n",
    "    slice_axis=slice_axis,\n",
    "    verbose=False  # Set to True for detailed data generation logs\n",
    ")\n",
    "\n",
    "# Create data loader for efficient batch processing\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size,  # Process multiple bones per batch\n",
    "    shuffle=True            # Randomize bone order each epoch\n",
    ")\n",
    "\n",
    "# Initialize generative network\n",
    "model = SimpleMLP(\n",
    "    input_dim=2,           # 2D coordinates (x, z)\n",
    "    latent_dim=latent_dim, # Shape embedding size\n",
    "    hidden_dim=32,         # Network capacity\n",
    "    output_dim=1           # Single SDF output\n",
    ").to(DEVICE)\n",
    "\n",
    "# Setup optimization\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.L1Loss()  # L1 loss for sharp surface reconstruction\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING LOOP: Learn generative SDF representation\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüöÄ Starting generative SDF training...\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Process each batch (contains multiple bones)\n",
    "    for batch_idx, (bone_indices, xz_coords, sdf_targets) in enumerate(dataloader):\n",
    "        # move batch data to device\n",
    "        bone_indices =  bone_indices.to(DEVICE)\n",
    "        xz_coords = xz_coords.to(DEVICE)\n",
    "        sdf_targets = sdf_targets.to(DEVICE)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # STEP 1: Lookup latent embeddings for bones in this batch\n",
    "        latents = lat_vecs(bone_indices)  # Shape: (batch_size, latent_dim)\n",
    "        \n",
    "        # STEP 2: Expand latents to match coordinate samples \n",
    "        # We need the same latent vector for all points from the same bone\n",
    "        latents = latents.unsqueeze(1).expand(-1, n_samples, -1)  # (batch, n_samples, latent_dim)\n",
    "        \n",
    "        # STEP 3: Predict SDF values using coordinates + shape embeddings\n",
    "        pred_sdf = model(xz_coords, latents)\n",
    "        \n",
    "        # STEP 4: Compute loss components\n",
    "        # SDF reconstruction loss - how well we predict signed distances\n",
    "        sdf_loss = criterion(pred_sdf, sdf_targets)\n",
    "        \n",
    "        # Latent regularization - prevent embeddings from growing too large\n",
    "        lat_loss = torch.sum(torch.norm(latents, dim=-1))\n",
    "        \n",
    "        # Combined loss: reconstruction + regularization\n",
    "        total_loss = sdf_loss + lat_loss\n",
    "        \n",
    "        # STEP 5: Backpropagation and parameter updates\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss statistics\n",
    "        running_loss += total_loss.item() * xz_coords.size(0)\n",
    "    \n",
    "    # Calculate epoch statistics\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    \n",
    "    # Progress reporting\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1:5d}/{num_epochs:,} | Loss: {epoch_loss:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22745a4e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: Evaluating the Generative Model\n",
    "\n",
    "### üéØ From Training to Generation\n",
    "\n",
    "Your generative SDF model is now trained. Unlike Tutorial #1 where we learned one specific bone, we now have a model that can:\n",
    "\n",
    "1. **Reconstruct any of the 5 training bones**\n",
    "2. **Interpolate between different bone shapes**  \n",
    "3. **Generate entirely new bone variations**\n",
    "\n",
    "### üé® Visualization Strategy\n",
    "\n",
    "We'll evaluate our model by:\n",
    "\n",
    "**Grid-Based SDF Evaluation**:\n",
    "- Create dense 2D grid of query points\n",
    "- For a specific bone, lookup its latent embedding\n",
    "- Predict SDF values across the entire grid\n",
    "- Visualize the learned SDF field\n",
    "\n",
    "**Surface Reconstruction**:\n",
    "- Use marching cubes to extract zero level set\n",
    "- Compare reconstructed surface to original bone\n",
    "- Assess geometric accuracy and smoothness\n",
    "\n",
    "### üîß Model Evaluation Process\n",
    "\n",
    "```python\n",
    "# STEP 1: Choose which bone to reconstruct\n",
    "LATENT_IDX = 0  # Bone index (0 to 4 in our case)\n",
    "\n",
    "# STEP 2: Get the learned latent embedding for this bone  \n",
    "latent_code = embedding_layer(LATENT_IDX)\n",
    "\n",
    "# STEP 3: Create grid of query coordinates\n",
    "grid_coordinates = create_coordinate_grid()\n",
    "\n",
    "# STEP 4: Predict SDF values using coordinates + latent code\n",
    "sdf_predictions = model(grid_coordinates, latent_code)\n",
    "\n",
    "# STEP 5: Visualize and reconstruct\n",
    "visualize_sdf_field(sdf_predictions)\n",
    "reconstruct_surface(sdf_predictions)\n",
    "```\n",
    "\n",
    "### üéõÔ∏è Key Parameters\n",
    "\n",
    "**Grid Resolution**: Controls detail level of reconstruction\n",
    "- Higher resolution ‚Üí more detailed surfaces, slower computation\n",
    "- Lower resolution ‚Üí faster computation, less detail\n",
    "\n",
    "**Latent Index**: Which bone to reconstruct\n",
    "- `LATENT_IDX = 0` ‚Üí First bone (reference)\n",
    "- `LATENT_IDX = 1` ‚Üí Second bone  \n",
    "- etc.\n",
    "\n",
    "### üöÄ What to Look For\n",
    "\n",
    "**Successful Learning Indicators**:\n",
    "- SDF field smoothly transitions from negative (inside) to positive (outside)\n",
    "- Zero level set closely matches original bone shape\n",
    "- Different latent codes produce different bone geometries\n",
    "- No artifacts or discontinuities in the SDF field\n",
    "\n",
    "**Quality Assessment**:\n",
    "- **Geometric accuracy**: How well does reconstruction match original?\n",
    "- **Surface smoothness**: Are there any jagged artifacts?\n",
    "- **Shape diversity**: Do different bones look appropriately different?\n",
    "\n",
    "Let's evaluate our trained generative model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e3252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4a80aa1c0b4f6189a130e20ae73b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(geometries=[{'vtkClass': 'vtkPolyData', 'points': {'vtkClass': 'vtkPoints', 'name': '_points', 'numberO‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SDF FIELD VISUALIZATION: Evaluate learned generative representation\n",
    "# =============================================================================\n",
    "\n",
    "# Choose which bone to reconstruct (0 to 4 for our 5-bone dataset)\n",
    "LATENT_IDX = 0\n",
    "\n",
    "# =============================================================================\n",
    "# GRID GENERATION: Create dense coordinate sampling for SDF evaluation\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Create high-resolution coordinate grid for detailed SDF field visualization\n",
    "GRID_STEP = 0.01  # Grid resolution (smaller = more detailed, slower)\n",
    "x = np.arange(-1, 1.01, GRID_STEP)\n",
    "z = np.arange(-1, 1.01, GRID_STEP)\n",
    "\n",
    "# Create coordinate meshgrid and flatten for network evaluation\n",
    "xx, zz = np.meshgrid(x, z)\n",
    "xz_grid = np.stack([xx.ravel(), zz.ravel()], axis=1)\n",
    "\n",
    "# Convert to PyTorch tensor for model inference  \n",
    "xz_tensor = torch.from_numpy(xz_grid).float().to(DEVICE)\n",
    "\n",
    "# =============================================================================\n",
    "# GENERATIVE SDF PREDICTION: Query network with specific latent code\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Disable gradient computation for inference (faster, less memory)\n",
    "with torch.no_grad():\n",
    "    # STEP 1: Lookup learned latent embedding for the specified bone\n",
    "    latent = lat_vecs(torch.tensor(LATENT_IDX)).to(DEVICE)\n",
    "    \n",
    "    # STEP 2: Expand latent to match number of query points\n",
    "    # Each coordinate gets the same latent code (identifies the bone shape)\n",
    "    latent_expanded = latent.expand(xz_tensor.size(0), -1)\n",
    "    \n",
    "    # STEP 3: Predict SDF values across the entire grid\n",
    "    sdf_pred = model(xz_tensor, latent_expanded).cpu().numpy().flatten()\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION PREPARATION: Convert predictions to 3D point cloud\n",
    "# =============================================================================\n",
    "\n",
    "# Convert 2D grid coordinates to 3D points (x, 0, z) for visualization\n",
    "# Y = 0 since we're working with a slice through the bone\n",
    "xyz_coords = np.stack([\n",
    "    xz_grid[:, 0],                    # X coordinates  \n",
    "    np.zeros_like(xz_grid[:, 0]),     # Y = 0 (slice level)\n",
    "    xz_grid[:, 1]                     # Z coordinates\n",
    "], axis=1)\n",
    "\n",
    "# Create PyVista point cloud for interactive visualization\n",
    "xyz_points = pv.PolyData(xyz_coords)\n",
    "\n",
    "# Assign predicted SDF values as point data for color mapping\n",
    "xyz_points['Predicted_SDF'] = sdf_pred\n",
    "\n",
    "# =============================================================================\n",
    "# INTERACTIVE VISUALIZATION: Compare learned vs. original bone\n",
    "# =============================================================================\n",
    "\n",
    "# Get original slice for comparison\n",
    "original_slice = list_tibs[LATENT_IDX].slice('y', origin=(0,0,0))\n",
    "\n",
    "# Create interactive 3D visualization comparing:\n",
    "# 1. Original bone slice (wireframe)\n",
    "# 2. Original full bone (solid)\n",
    "# 3. Learned SDF field (colored points)\n",
    "view(\n",
    "    geometries=[original_slice, list_tibs[LATENT_IDX]], \n",
    "    point_sets=xyz_points, \n",
    "    point_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce66cd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7: Generative Surface Reconstruction\n",
    "\n",
    "### üéØ From SDF Field to 3D Surface\n",
    "\n",
    "The previous visualization showed us the raw SDF field as colored points. Now we'll extract the actual bone surface using marching cubes. \n",
    "\n",
    "### üé® Reconstruction Process\n",
    "\n",
    "**Our Approach**:\n",
    "1. **Setup 3D grid** for marching cubes algorithm\n",
    "2. **Choose target bone** via latent index\n",
    "3. **Predict SDF field** using coordinates + latent embedding\n",
    "4. **Extract zero level set** where SDF transitions from negative to positive\n",
    "5. **Compare reconstruction** with original bone geometry\n",
    "\n",
    "### üß™ Experimental Possibilities\n",
    "\n",
    "Once you understand this process, try:\n",
    "\n",
    "**Explore Different Bones**:\n",
    "```python\n",
    "LATENT_IDX = 0  # First bone\n",
    "LATENT_IDX = 1  # Second bone  \n",
    "LATENT_IDX = 2  # Third bone\n",
    "# etc.\n",
    "```\n",
    "\n",
    "**Shape Interpolation** (advanced):\n",
    "```python\n",
    "latent_A = lat_vecs(torch.tensor(0))  # First bone\n",
    "latent_B = lat_vecs(torch.tensor(1))  # Second bone\n",
    "latent_interp = 0.5 * latent_A + 0.5 * latent_B  # 50/50 blend\n",
    "# Use latent_interp for reconstruction ‚Üí hybrid bone!\n",
    "```\n",
    "\n",
    "**Novel Shape Generation** (advanced):\n",
    "```python\n",
    "latent_new = torch.randn(latent_dim) * 0.1  # Random latent code\n",
    "# Use latent_new for reconstruction ‚Üí entirely new bone variation!\n",
    "```\n",
    "\n",
    "### üéØ Quality Assessment\n",
    "\n",
    "**What to Look For**:\n",
    "- **Surface smoothness**: Clean, artifact-free reconstruction\n",
    "- **Anatomical accuracy**: Preserved bone structure and proportions\n",
    "- **Shape diversity**: Different bones should look appropriately different\n",
    "- **Boundary precision**: Sharp transitions between inside/outside regions\n",
    "\n",
    "### üìä Reconstruction Parameters\n",
    "\n",
    "**Grid Resolution**: Balance between detail and computation time\n",
    "- `n = 100` ‚Üí Quick preview reconstruction  \n",
    "- `n = 200` ‚Üí High-detail reconstruction (slower)\n",
    "\n",
    "**Spatial Bounds**: Ensure we cover the bone region\n",
    "- `[-1, 1]` range matches our normalized coordinate system\n",
    "\n",
    "Let's extract our generative bone surface!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f74d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Grid resolution: 100 √ó 2 √ó 100 = 20,000 total points\n",
      "   ‚Ä¢ Spatial coverage: X[-1.0, 1.0], Y[-0.01, 0.01], Z[-1.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b7628a830b4c15b255b0eac766f3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(geometries=[{'vtkClass': 'vtkPolyData', 'points': {'vtkClass': 'vtkPoints', 'name': '_points', 'numberO‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# GENERATIVE SURFACE RECONSTRUCTION: Extract bone geometry from learned SDF\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# MARCHING CUBES GRID SETUP: Create structured 3D grid for surface extraction\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Grid resolution for surface extraction (balance detail vs. computation)\n",
    "n = 100  # Grid points per dimension (higher = more detail, slower)\n",
    "\n",
    "# Spatial bounds matching our normalized coordinate system\n",
    "x_min, y_min, z_min = -1.0, -0.01, -1.0   # Minimum bounds\n",
    "x_max, y_max, z_max =  1.0,  0.01,  1.0    # Maximum bounds\n",
    "\n",
    "print(f\"   ‚Ä¢ Grid resolution: {n} √ó 2 √ó {n} = {n*2*n:,} total points\")\n",
    "print(f\"   ‚Ä¢ Spatial coverage: X[{x_min}, {x_max}], Y[{y_min}, {y_max}], Z[{z_min}, {z_max}]\")\n",
    "\n",
    "# Calculate grid spacing\n",
    "spacing = (\n",
    "    (x_max - x_min) / (n - 1),  # X direction spacing\n",
    "    0.01,                       # Y direction spacing (thin slice)\n",
    "    (z_max - z_min) / (n - 1),  # Z direction spacing\n",
    ")\n",
    "\n",
    "# Create structured grid for marching cubes algorithm\n",
    "grid = pv.ImageData(\n",
    "    dimensions=(n, 2, n),     # Grid dimensions\n",
    "    spacing=spacing,          # Point spacing\n",
    "    origin=(x_min, y_min, z_min),  # Grid origin\n",
    ")\n",
    "\n",
    "# Extract grid coordinates for SDF evaluation\n",
    "x, y, z = grid.points.T\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# GENERATIVE SDF EVALUATION: Predict signed distances across the grid\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Prepare coordinate input for the neural network\n",
    "# Extract (x, z) coordinates since we're working with 2D slices\n",
    "xz_grid_3d = np.stack([x, z], axis=1)\n",
    "xz_tensor_3d = torch.from_numpy(xz_grid_3d).float().to(DEVICE)\n",
    "\n",
    "\n",
    "# Predict SDF values using the trained generative model\n",
    "with torch.no_grad():\n",
    "    # Get the learned latent embedding for the target bone\n",
    "    latent = lat_vecs(torch.tensor(LATENT_IDX)).to(DEVICE)\n",
    "    \n",
    "    # Expand latent to match all grid points (same bone identity for all points)\n",
    "    latent_expanded = latent.expand(xz_tensor_3d.size(0), -1)\n",
    "    \n",
    "    # Generate SDF predictions across the entire grid\n",
    "    sdf_pred_3d = model(xz_tensor_3d, latent_expanded).cpu().numpy().flatten()\n",
    "    \n",
    "# =============================================================================\n",
    "# MARCHING CUBES: Extract zero level set as triangulated surface\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Extract isosurface where SDF = 0 (the bone boundary)\n",
    "# This creates a triangulated mesh representing the reconstructed bone surface\n",
    "reconstructed_mesh = grid.contour([0], sdf_pred_3d, method='marching_cubes')\n",
    "\n",
    "# Compute surface normals for proper lighting and visualization\n",
    "reconstructed_mesh.compute_normals(inplace=True)\n",
    "\n",
    "# =============================================================================\n",
    "# COMPARISON PREPARATION: Get original bone slice for reference\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Extract slice from the original bone for visual comparison\n",
    "original_slice = list_tibs[LATENT_IDX].slice('y', origin=(0, 0, 0))\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL VISUALIZATION: Compare reconstruction with original\n",
    "# =============================================================================\n",
    "\n",
    "# Create interactive visualization comparing:\n",
    "# 1. Generative reconstruction (colored surface)\n",
    "# 2. Original bone slice (wireframe reference)\n",
    "view(geometries=[reconstructed_mesh, original_slice])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42321e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 8: Exploring the Shape Space - Mean Shape Generation\n",
    "\n",
    "### üéØ Beyond Individual Reconstruction\n",
    "\n",
    "So far, we've reconstructed specific bones using their individual latent codes. But our generative model enables something even more powerful: **shape space exploration**!\n",
    "\n",
    "### üß¨ The Concept of \"Average\" Shapes\n",
    "\n",
    "One of the most interesting properties of latent shape spaces is the ability to compute **canonical** or **average** shapes:\n",
    "\n",
    "```python\n",
    "# Individual bone shapes:\n",
    "bone_0 = f(coordinates, latent_vector_0)  # Patient A's specific anatomy\n",
    "bone_1 = f(coordinates, latent_vector_1)  # Patient B's specific anatomy\n",
    "\n",
    "# Average bone shape:\n",
    "mean_latent = (latent_vector_0 + latent_vector_1 + ...) / num_bones\n",
    "average_bone = f(coordinates, mean_latent)  # Population average anatomy!\n",
    "```\n",
    "\n",
    "### üî¨ Why Average Shapes Matter\n",
    "\n",
    "**Statistical Shape Analysis**:\n",
    "- **Population norms**: What does a \"typical\" tibia look like?\n",
    "- **Pathology detection**: How far is a patient's bone from the population average?\n",
    "- **Reference templates**: Standardized anatomy for medical education\n",
    "\n",
    "**Clinical Applications**:\n",
    "- **Surgical planning**: Start with average anatomy, then personalize\n",
    "- **Implant design**: Design for the population average, then customize\n",
    "- **Growth modeling**: Track how individual bones deviate from population means\n",
    "\n",
    "**Research Applications**:\n",
    "- **Shape variation studies**: Understanding anatomical diversity\n",
    "- **Developmental biology**: How shapes change with age/development\n",
    "- **Comparative anatomy**: Differences between populations or species\n",
    "\n",
    "\n",
    "\n",
    "Let's generate and visualize the population average tibia!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57151f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07623f1bbe874f549b5a1c7a8e3bd2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(geometries=[{'vtkClass': 'vtkPolyData', 'points': {'vtkClass': 'vtkPoints', 'name': '_points', 'numberO‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MEAN SHAPE GENERATION: Explore the center of our learned shape space\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# LATENT SPACE ANALYSIS: Compute statistics of learned shape embeddings\n",
    "# =============================================================================\n",
    "\n",
    "# Extract all learned latent embeddings for analysis\n",
    "all_latents = lat_vecs.weight.data  # Shape: (num_bones, latent_dim)\n",
    "\n",
    "# Compute latent space statistics\n",
    "latent_mean = all_latents.mean(dim=0, keepdim=True)  # Population center\n",
    "latent_std = all_latents.std(dim=0)                  # Per-dimension variation\n",
    "latent_range = all_latents.max(dim=0)[0] - all_latents.min(dim=0)[0]  # Value ranges\n",
    "\n",
    "# =============================================================================\n",
    "# RECONSTRUCTION GRID SETUP: Same spatial domain as individual bones\n",
    "# =============================================================================\n",
    "\n",
    "# Use same grid parameters as individual bone reconstruction for comparison\n",
    "n = 100  # Grid resolution\n",
    "x_min, y_min, z_min = -1.0, -0.01, -1.0   # Spatial bounds\n",
    "x_max, y_max, z_max =  1.0,  0.01,  1.0\n",
    "\n",
    "# Create reconstruction grid\n",
    "spacing = (\n",
    "    (x_max - x_min) / (n - 1),  # X spacing\n",
    "    0.01,                       # Y spacing (thin slice)  \n",
    "    (z_max - z_min) / (n - 1),  # Z spacing\n",
    ")\n",
    "\n",
    "grid = pv.ImageData(\n",
    "    dimensions=(n, 2, n),\n",
    "    spacing=spacing,\n",
    "    origin=(x_min, y_min, z_min),\n",
    ")\n",
    "\n",
    "x, y, z = grid.points.T\n",
    "\n",
    "# =============================================================================\n",
    "# MEAN SHAPE SDF GENERATION: Use population average latent code\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Prepare spatial coordinates for network evaluation\n",
    "xz_grid_3d = np.stack([x, z], axis=1)\n",
    "xz_tensor_3d = torch.from_numpy(xz_grid_3d).float().to(DEVICE)\n",
    "\n",
    "# Generate SDF predictions using the MEAN of all learned latent embeddings\n",
    "with torch.no_grad():\n",
    "    # CRITICAL: Use mean of all latent vectors ‚Üí \"average\" bone shape\n",
    "    # This represents the center point of our learned shape space\n",
    "    latent_mean = lat_vecs.weight.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    # Expand mean latent to match all grid points\n",
    "    latent_expanded = latent_mean.expand(xz_tensor_3d.size(0), -1)\n",
    "    \n",
    "    # Predict SDF field for the \"average\" bone\n",
    "    sdf_pred_3d = model(xz_tensor_3d, latent_expanded).cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SURFACE EXTRACTION: Extract average bone geometry\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Extract zero level set ‚Üí average bone surface\n",
    "mean_bone_mesh = grid.contour([0], sdf_pred_3d, method='marching_cubes')\n",
    "mean_bone_mesh.compute_normals(inplace=True)\n",
    "\n",
    "# =============================================================================\n",
    "# COMPARISON VISUALIZATION: Mean vs. individual reference bone\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Create visualization showing:\n",
    "# 1. Population average bone (generated from mean latent)\n",
    "# 2. Reference individual bone slice (for comparison)\n",
    "original_slice = list_tibs[0].slice('y')  # Use first bone as reference\n",
    "view(geometries=[mean_bone_mesh, original_slice])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cee925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Mesh is now synonymous with pyvista.PolyData and thus this property is redundant and the Mesh object can be used for anything that pyvista.PolyData or vtk.vtkPolyData can be used for.\n",
      "Epoch 1/500, Loss: 0.0105\n",
      "Latent norm: 0.0377\n",
      "Epoch 2/500, Loss: 0.0109\n",
      "Latent norm: 0.0376\n",
      "Epoch 3/500, Loss: 0.0104\n",
      "Latent norm: 0.0385\n",
      "Epoch 4/500, Loss: 0.0093\n",
      "Latent norm: 0.0432\n",
      "Epoch 5/500, Loss: 0.0098\n",
      "Latent norm: 0.0497\n",
      "Epoch 6/500, Loss: 0.0099\n",
      "Latent norm: 0.0536\n",
      "Epoch 7/500, Loss: 0.0094\n",
      "Latent norm: 0.0558\n",
      "Epoch 8/500, Loss: 0.0094\n",
      "Latent norm: 0.0568\n",
      "Epoch 9/500, Loss: 0.0094\n",
      "Latent norm: 0.0563\n",
      "Epoch 10/500, Loss: 0.0092\n",
      "Latent norm: 0.0554\n",
      "Epoch 11/500, Loss: 0.0088\n",
      "Latent norm: 0.0555\n",
      "Epoch 12/500, Loss: 0.0086\n",
      "Latent norm: 0.0571\n",
      "Epoch 13/500, Loss: 0.0085\n",
      "Latent norm: 0.0599\n",
      "Epoch 14/500, Loss: 0.0085\n",
      "Latent norm: 0.0615\n",
      "Epoch 15/500, Loss: 0.0083\n",
      "Latent norm: 0.0629\n",
      "Epoch 16/500, Loss: 0.0082\n",
      "Latent norm: 0.0655\n",
      "Epoch 17/500, Loss: 0.0079\n",
      "Latent norm: 0.0683\n",
      "Epoch 18/500, Loss: 0.0077\n",
      "Latent norm: 0.0713\n",
      "Epoch 19/500, Loss: 0.0076\n",
      "Latent norm: 0.0750\n",
      "Epoch 20/500, Loss: 0.0075\n",
      "Latent norm: 0.0784\n",
      "Epoch 21/500, Loss: 0.0073\n",
      "Latent norm: 0.0818\n",
      "Epoch 22/500, Loss: 0.0072\n",
      "Latent norm: 0.0859\n",
      "Epoch 23/500, Loss: 0.0069\n",
      "Latent norm: 0.0905\n",
      "Epoch 24/500, Loss: 0.0068\n",
      "Latent norm: 0.0950\n",
      "Epoch 25/500, Loss: 0.0066\n",
      "Latent norm: 0.0985\n",
      "Epoch 26/500, Loss: 0.0066\n",
      "Latent norm: 0.1014\n",
      "Epoch 27/500, Loss: 0.0065\n",
      "Latent norm: 0.1051\n",
      "Epoch 28/500, Loss: 0.0064\n",
      "Latent norm: 0.1092\n",
      "Epoch 29/500, Loss: 0.0063\n",
      "Latent norm: 0.1134\n",
      "Epoch 30/500, Loss: 0.0062\n",
      "Latent norm: 0.1166\n",
      "Epoch 31/500, Loss: 0.0061\n",
      "Latent norm: 0.1192\n",
      "Epoch 32/500, Loss: 0.0061\n",
      "Latent norm: 0.1222\n",
      "Epoch 33/500, Loss: 0.0059\n",
      "Latent norm: 0.1255\n",
      "Epoch 34/500, Loss: 0.0060\n",
      "Latent norm: 0.1290\n",
      "Epoch 35/500, Loss: 0.0059\n",
      "Latent norm: 0.1314\n",
      "Epoch 36/500, Loss: 0.0059\n",
      "Latent norm: 0.1329\n",
      "Epoch 37/500, Loss: 0.0059\n",
      "Latent norm: 0.1345\n",
      "Epoch 38/500, Loss: 0.0058\n",
      "Latent norm: 0.1361\n",
      "Epoch 39/500, Loss: 0.0059\n",
      "Latent norm: 0.1378\n",
      "Epoch 40/500, Loss: 0.0058\n",
      "Latent norm: 0.1388\n",
      "Epoch 41/500, Loss: 0.0059\n",
      "Latent norm: 0.1396\n",
      "Epoch 42/500, Loss: 0.0058\n",
      "Latent norm: 0.1404\n",
      "Epoch 43/500, Loss: 0.0058\n",
      "Latent norm: 0.1411\n",
      "Epoch 44/500, Loss: 0.0058\n",
      "Latent norm: 0.1411\n",
      "Epoch 45/500, Loss: 0.0057\n",
      "Latent norm: 0.1406\n",
      "Epoch 46/500, Loss: 0.0058\n",
      "Latent norm: 0.1397\n",
      "Epoch 47/500, Loss: 0.0057\n",
      "Latent norm: 0.1392\n",
      "Epoch 48/500, Loss: 0.0057\n",
      "Latent norm: 0.1388\n",
      "Epoch 49/500, Loss: 0.0057\n",
      "Latent norm: 0.1381\n",
      "Epoch 50/500, Loss: 0.0056\n",
      "Latent norm: 0.1371\n",
      "Epoch 51/500, Loss: 0.0056\n",
      "Latent norm: 0.1360\n",
      "Epoch 52/500, Loss: 0.0055\n",
      "Latent norm: 0.1354\n",
      "Epoch 53/500, Loss: 0.0055\n",
      "Latent norm: 0.1350\n",
      "Epoch 54/500, Loss: 0.0055\n",
      "Latent norm: 0.1346\n",
      "Epoch 55/500, Loss: 0.0055\n",
      "Latent norm: 0.1340\n",
      "Epoch 56/500, Loss: 0.0055\n",
      "Latent norm: 0.1336\n",
      "Epoch 57/500, Loss: 0.0055\n",
      "Latent norm: 0.1332\n",
      "Epoch 58/500, Loss: 0.0055\n",
      "Latent norm: 0.1329\n",
      "Epoch 59/500, Loss: 0.0055\n",
      "Latent norm: 0.1328\n",
      "Epoch 60/500, Loss: 0.0054\n",
      "Latent norm: 0.1330\n",
      "Epoch 61/500, Loss: 0.0054\n",
      "Latent norm: 0.1334\n",
      "Epoch 62/500, Loss: 0.0054\n",
      "Latent norm: 0.1337\n",
      "Epoch 63/500, Loss: 0.0054\n",
      "Latent norm: 0.1339\n",
      "Epoch 64/500, Loss: 0.0055\n",
      "Latent norm: 0.1341\n",
      "Epoch 65/500, Loss: 0.0055\n",
      "Latent norm: 0.1345\n",
      "Epoch 66/500, Loss: 0.0054\n",
      "Latent norm: 0.1349\n",
      "Epoch 67/500, Loss: 0.0054\n",
      "Latent norm: 0.1354\n",
      "Epoch 68/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 69/500, Loss: 0.0054\n",
      "Latent norm: 0.1360\n",
      "Epoch 70/500, Loss: 0.0055\n",
      "Latent norm: 0.1361\n",
      "Epoch 71/500, Loss: 0.0054\n",
      "Latent norm: 0.1362\n",
      "Epoch 72/500, Loss: 0.0055\n",
      "Latent norm: 0.1364\n",
      "Epoch 73/500, Loss: 0.0054\n",
      "Latent norm: 0.1367\n",
      "Epoch 74/500, Loss: 0.0054\n",
      "Latent norm: 0.1371\n",
      "Epoch 75/500, Loss: 0.0054\n",
      "Latent norm: 0.1374\n",
      "Epoch 76/500, Loss: 0.0054\n",
      "Latent norm: 0.1377\n",
      "Epoch 77/500, Loss: 0.0054\n",
      "Latent norm: 0.1378\n",
      "Epoch 78/500, Loss: 0.0054\n",
      "Latent norm: 0.1377\n",
      "Epoch 79/500, Loss: 0.0054\n",
      "Latent norm: 0.1376\n",
      "Epoch 80/500, Loss: 0.0054\n",
      "Latent norm: 0.1376\n",
      "Epoch 81/500, Loss: 0.0054\n",
      "Latent norm: 0.1377\n",
      "Epoch 82/500, Loss: 0.0054\n",
      "Latent norm: 0.1378\n",
      "Epoch 83/500, Loss: 0.0054\n",
      "Latent norm: 0.1377\n",
      "Epoch 84/500, Loss: 0.0054\n",
      "Latent norm: 0.1377\n",
      "Epoch 85/500, Loss: 0.0054\n",
      "Latent norm: 0.1375\n",
      "Epoch 86/500, Loss: 0.0054\n",
      "Latent norm: 0.1374\n",
      "Epoch 87/500, Loss: 0.0054\n",
      "Latent norm: 0.1373\n",
      "Epoch 88/500, Loss: 0.0054\n",
      "Latent norm: 0.1371\n",
      "Epoch 89/500, Loss: 0.0054\n",
      "Latent norm: 0.1370\n",
      "Epoch 90/500, Loss: 0.0054\n",
      "Latent norm: 0.1369\n",
      "Epoch 91/500, Loss: 0.0054\n",
      "Latent norm: 0.1368\n",
      "Epoch 92/500, Loss: 0.0054\n",
      "Latent norm: 0.1367\n",
      "Epoch 93/500, Loss: 0.0054\n",
      "Latent norm: 0.1366\n",
      "Epoch 94/500, Loss: 0.0054\n",
      "Latent norm: 0.1364\n",
      "Epoch 95/500, Loss: 0.0054\n",
      "Latent norm: 0.1363\n",
      "Epoch 96/500, Loss: 0.0054\n",
      "Latent norm: 0.1362\n",
      "Epoch 97/500, Loss: 0.0054\n",
      "Latent norm: 0.1360\n",
      "Epoch 98/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 99/500, Loss: 0.0054\n",
      "Latent norm: 0.1357\n",
      "Epoch 100/500, Loss: 0.0054\n",
      "Latent norm: 0.1355\n",
      "Epoch 101/500, Loss: 0.0054\n",
      "Latent norm: 0.1354\n",
      "Epoch 102/500, Loss: 0.0054\n",
      "Latent norm: 0.1354\n",
      "Epoch 103/500, Loss: 0.0054\n",
      "Latent norm: 0.1354\n",
      "Epoch 104/500, Loss: 0.0054\n",
      "Latent norm: 0.1354\n",
      "Epoch 105/500, Loss: 0.0054\n",
      "Latent norm: 0.1354\n",
      "Epoch 106/500, Loss: 0.0054\n",
      "Latent norm: 0.1354\n",
      "Epoch 107/500, Loss: 0.0054\n",
      "Latent norm: 0.1355\n",
      "Epoch 108/500, Loss: 0.0054\n",
      "Latent norm: 0.1355\n",
      "Epoch 109/500, Loss: 0.0054\n",
      "Latent norm: 0.1355\n",
      "Epoch 110/500, Loss: 0.0054\n",
      "Latent norm: 0.1356\n",
      "Epoch 111/500, Loss: 0.0054\n",
      "Latent norm: 0.1356\n",
      "Epoch 112/500, Loss: 0.0054\n",
      "Latent norm: 0.1356\n",
      "Epoch 113/500, Loss: 0.0054\n",
      "Latent norm: 0.1356\n",
      "Epoch 114/500, Loss: 0.0054\n",
      "Latent norm: 0.1355\n",
      "Epoch 115/500, Loss: 0.0054\n",
      "Latent norm: 0.1356\n",
      "Epoch 116/500, Loss: 0.0054\n",
      "Latent norm: 0.1355\n",
      "Epoch 117/500, Loss: 0.0054\n",
      "Latent norm: 0.1356\n",
      "Epoch 118/500, Loss: 0.0054\n",
      "Latent norm: 0.1357\n",
      "Epoch 119/500, Loss: 0.0054\n",
      "Latent norm: 0.1356\n",
      "Epoch 120/500, Loss: 0.0054\n",
      "Latent norm: 0.1356\n",
      "Epoch 121/500, Loss: 0.0054\n",
      "Latent norm: 0.1356\n",
      "Epoch 122/500, Loss: 0.0054\n",
      "Latent norm: 0.1357\n",
      "Epoch 123/500, Loss: 0.0054\n",
      "Latent norm: 0.1357\n",
      "Epoch 124/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 125/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 126/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 127/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 128/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 129/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 130/500, Loss: 0.0054\n",
      "Latent norm: 0.1360\n",
      "Epoch 131/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 132/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 133/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 134/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 135/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 136/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 137/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 138/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 139/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 140/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 141/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 142/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 143/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 144/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 145/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 146/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 147/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 148/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 149/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 150/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 151/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 152/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 153/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 154/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 155/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 156/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 157/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 158/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 159/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 160/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 161/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 162/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 163/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 164/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 165/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 166/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 167/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 168/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 169/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 170/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 171/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 172/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 173/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 174/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 175/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 176/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 177/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 178/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 179/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 180/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 181/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 182/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 183/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 184/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 185/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 186/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 187/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 188/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 189/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 190/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 191/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 192/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 193/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 194/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 195/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 196/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 197/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 198/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 199/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 200/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 201/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 202/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 203/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 204/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 205/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 206/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 207/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 208/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 209/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 210/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 211/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 212/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 213/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 214/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 215/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 216/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 217/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 218/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 219/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 220/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 221/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 222/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 223/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 224/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 225/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 226/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 227/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 228/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 229/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 230/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 231/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 232/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 233/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 234/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 235/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 236/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 237/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 238/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 239/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 240/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 241/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 242/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 243/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 244/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 245/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 246/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 247/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 248/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 249/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 250/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 251/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 252/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 253/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 254/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 255/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 256/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 257/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 258/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 259/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 260/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 261/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 262/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 263/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 264/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 265/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 266/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 267/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 268/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 269/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 270/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 271/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 272/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 273/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 274/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 275/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 276/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 277/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 278/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 279/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 280/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 281/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 282/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 283/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 284/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 285/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 286/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 287/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 288/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 289/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 290/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 291/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 292/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 293/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 294/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 295/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 296/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 297/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 298/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 299/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 300/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 301/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 302/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 303/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 304/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 305/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 306/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 307/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 308/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 309/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 310/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 311/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 312/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 313/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 314/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 315/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 316/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 317/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 318/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 319/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 320/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 321/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 322/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 323/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 324/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 325/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 326/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 327/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 328/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 329/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 330/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 331/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 332/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 333/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 334/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 335/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 336/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 337/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 338/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 339/500, Loss: 0.0054\n",
      "Latent norm: 0.1359\n",
      "Epoch 340/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 341/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 342/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 343/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 344/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 345/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 346/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 347/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 348/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 349/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 350/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 351/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 352/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 353/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 354/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 355/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 356/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 357/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 358/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 359/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 360/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 361/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 362/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 363/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 364/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 365/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 366/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 367/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 368/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 369/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 370/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 371/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 372/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 373/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 374/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 375/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 376/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 377/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 378/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 379/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 380/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 381/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 382/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 383/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 384/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 385/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 386/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 387/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 388/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 389/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 390/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 391/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 392/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 393/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 394/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 395/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 396/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 397/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 398/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 399/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 400/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 401/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 402/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 403/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 404/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 405/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 406/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 407/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 408/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 409/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 410/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 411/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 412/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 413/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 414/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 415/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 416/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 417/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 418/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 419/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 420/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 421/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 422/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 423/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 424/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 425/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 426/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 427/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 428/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 429/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 430/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 431/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 432/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 433/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 434/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 435/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 436/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 437/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 438/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 439/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 440/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 441/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 442/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 443/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 444/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 445/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 446/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 447/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 448/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 449/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 450/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 451/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 452/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 453/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 454/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 455/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 456/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 457/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 458/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 459/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 460/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 461/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 462/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 463/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 464/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 465/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 466/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 467/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 468/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 469/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 470/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 471/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 472/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 473/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 474/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 475/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 476/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 477/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 478/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 479/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 480/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 481/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 482/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 483/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 484/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 485/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 486/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 487/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 488/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 489/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 490/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 491/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 492/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 493/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 494/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 495/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 496/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 497/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 498/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 499/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n",
      "Epoch 500/500, Loss: 0.0054\n",
      "Latent norm: 0.1358\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1711ce9263514c878597442ec32de229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(geometries=[{'vtkClass': 'vtkPolyData', 'points': {'vtkClass': 'vtkPoints', 'name': '_points', 'numberO‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===========================\n",
    "# LOAD AND PREPARE TEST MESH\n",
    "# ===========================\n",
    "\n",
    "# Load in a new/test mesh (not seen during training)\n",
    "test_mesh = mskt.mesh.Mesh(list_tib_paths[-3])\n",
    "\n",
    "# Scale the test mesh so it's close to the size of the reference mesh\n",
    "normalize_bone(test_mesh)\n",
    "\n",
    "# Register the new mesh with the reference mesh (rigid registration)\n",
    "test_mesh.rigidly_register(ref_tibia, return_transform=True)\n",
    "\n",
    "# Extract a 2D slice from the registered/scaled mesh in the y direction\n",
    "test_slice = test_mesh.slice('y', origin=(0, 0, 0))\n",
    "\n",
    "# Get the slice's x and z coordinates (to match training input format)\n",
    "test_slice_points = torch.from_numpy(test_slice.points[:, [0, 2]]).float().to(DEVICE)\n",
    "\n",
    "# ==========================================\n",
    "# LATENT VECTOR INITIALIZATION AND OPTIMIZER\n",
    "# ==========================================\n",
    "\n",
    "# Create a new latent vector with random values close to zero (mean=0, std=0.02)\n",
    "latent = torch.ones(1, latent_dim).normal_(mean=0, std=0.01).to(DEVICE)\n",
    "latent.requires_grad = True  # Enable gradient computation for optimization\n",
    "\n",
    "# Set up optimizer to optimize ONLY the latent vector (not the network weights)\n",
    "optimizer = torch.optim.Adam([latent], lr=1e-2)\n",
    "\n",
    "# Training hyperparameters\n",
    "n_epochs = 500           # Number of optimization steps\n",
    "lr_decay = 0.99          # Learning rate decay per epoch\n",
    "latent_norm_weight = 1e-7   # Regularization weight for latent norm (set to small value here)\n",
    "\n",
    "# Lists to store loss and learning rate history for plotting\n",
    "losses = []\n",
    "lrs = []\n",
    "\n",
    "# ==========================================\n",
    "# OPTIMIZE LATENT VECTOR TO FIT TEST SLICE\n",
    "# ==========================================\n",
    "\n",
    "for iter in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Expand latent vector to match the number of slice points\n",
    "    latent_input = latent.expand(test_slice.points.shape[0], -1).float()\n",
    "    \n",
    "    # Input points from new slice into network, along with the new latent vector\n",
    "    # Predict the SDF values for these points\n",
    "    sdf_pred = model(test_slice_points, latent_input)\n",
    "    \n",
    "    # Compute L1 loss between predicted SDF and the ground truth SDF (which is 0 for the surface)\n",
    "    recon_loss = torch.mean(torch.abs(sdf_pred))\n",
    "    \n",
    "    # Optionally regularize the latent vector norm (encourage it to stay small/ more \"plausible\")\n",
    "    latent_loss = torch.norm(latent)\n",
    "    \n",
    "    # Total loss: reconstruction + (optionally) latent regularization\n",
    "    loss = recon_loss + latent_loss * latent_norm_weight\n",
    "    \n",
    "    # Backpropagate and update the latent vector\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print progress for monitoring\n",
    "    print(f\"Epoch {iter+1}/{n_epochs}, Loss: {loss.item():.4f}\")\n",
    "    print(f\"Latent norm: {latent_loss.item():.4f}\")\n",
    "    \n",
    "    # Store loss and learning rate for plotting\n",
    "    losses.append(loss.item())\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Decay the learning rate\n",
    "    optimizer.param_groups[0]['lr'] *= lr_decay\n",
    "\n",
    "# Plot the loss curve over optimization\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "\n",
    "# ==========================================================\n",
    "# RECONSTRUCTION GRID SETUP: MATCH TRAINING SPATIAL DOMAIN\n",
    "# ==========================================================\n",
    "\n",
    "# Use same grid parameters as individual bone reconstruction for comparison\n",
    "n = 100  # Grid resolution (number of points along x and z)\n",
    "x_min, y_min, z_min = -1.0, -0.01, -1.0   # Spatial bounds (match training)\n",
    "x_max, y_max, z_max =  1.0,  0.01,  1.0\n",
    "\n",
    "# Create a 3D grid for SDF evaluation (thin in y, dense in x and z)\n",
    "spacing = (\n",
    "    (x_max - x_min) / (n - 1),  # X spacing\n",
    "    0.01,                       # Y spacing (thin slice)  \n",
    "    (z_max - z_min) / (n - 1),  # Z spacing\n",
    ")\n",
    "\n",
    "grid = pv.ImageData(\n",
    "    dimensions=(n, 2, n),       # 2 voxels in y (thin)\n",
    "    spacing=spacing,\n",
    "    origin=(x_min, y_min, z_min),\n",
    ")\n",
    "\n",
    "# Extract grid coordinates for SDF prediction\n",
    "x, y, z = grid.points.T\n",
    "\n",
    "# ==========================================\n",
    "# GENERATE SDF FIELD USING OPTIMIZED LATENT\n",
    "# ==========================================\n",
    "\n",
    "# Prepare spatial coordinates for network evaluation (x and z only)\n",
    "xz_grid_3d = np.stack([x, z], axis=1)\n",
    "xz_tensor_3d = torch.from_numpy(xz_grid_3d).float()\n",
    "\n",
    "# Use the optimized latent vector to generate SDF predictions for the grid\n",
    "with torch.no_grad():\n",
    "    # Expand latent vector to match number of grid points\n",
    "    latent_ = latent.expand(xz_tensor_3d.size(0), -1).float()\n",
    "    # Predict SDF field for the fitted latent vector\n",
    "    sdf_pred_3d = model(xz_tensor_3d, latent_).cpu().numpy().flatten()\n",
    "\n",
    "# ==========================================\n",
    "# EXTRACT SURFACE FROM SDF FIELD\n",
    "# ==========================================\n",
    "\n",
    "# Extract the zero level set (isosurface) ‚Üí reconstructed bone surface\n",
    "fitted_test_bone_mesh = grid.contour([0], sdf_pred_3d, method='marching_cubes')\n",
    "fitted_test_bone_mesh.compute_normals(inplace=True)\n",
    "\n",
    "# ==========================================\n",
    "# VISUALIZE: ORIGINAL SLICE VS RECONSTRUCTION\n",
    "# ==========================================\n",
    "\n",
    "# Create visualization showing:\n",
    "# 1. Reconstructed bone surface (from optimized latent)\n",
    "# 2. Original test bone slice (for comparison)\n",
    "view(geometries=[fitted_test_bone_mesh, test_slice])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed40f1c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Tutorial Complete: Generative Shape Modeling Achieved!\n",
    "\n",
    "You've successfully built and trained a **generative neural shape model** that can represent multiple bone shapes in a single network.\n",
    "\n",
    "### Key Parts\n",
    "\n",
    "**1. Multi-Shape Learning** ü¶¥\n",
    "- Loaded and registered 5 different tibia bones from different patients\n",
    "- Aligned them into a consistent coordinate system for shared learning\n",
    "\n",
    "**2. Generative Architecture** üß†\n",
    "- Built latent embedding layer to encode shape identity\n",
    "- Enhanced MLP to process coordinates + shape embeddings\n",
    "- Learned both shared anatomical patterns AND individual variations\n",
    "\n",
    "**3. Advanced Training** üìà\n",
    "- Multi-shape dataset with bone identity tracking\n",
    "- Extended training (40K epochs) for generative learning\n",
    "- Regularized latent codes to encourage smooth shape space\n",
    "\n",
    "**4. Shape Reconstruction** üé®\n",
    "- Demonstrated reconstruction of specific bones via latent codes\n",
    "- Visualized learned SDF fields across spatial domains\n",
    "- Extracted high-quality surfaces using marching cubes\n",
    "\n",
    "**5. Shape Space Exploration** üß¨\n",
    "- Generated population average bone using latent arithmetic\n",
    "- Demonstrated canonical shape generation and latent space analysis\n",
    "- Explored the center of the learned shape space\n",
    "\n",
    "### üéØ From Tutorial #1 to Tutorial #2\n",
    "\n",
    "| Aspect | Tutorial #1 (Single Shape) | Tutorial #2 (Generative) |\n",
    "|--------|----------------------------|---------------------------|\n",
    "| **Input** | `f(x,z) ‚Üí SDF` | `f(x,z,latent) ‚Üí SDF` |\n",
    "| **Capacity** | One specific bone | Multiple bone shapes |\n",
    "| **Training Data** | 40K points from 1 bone | 100K+ points from 5 bones |\n",
    "| **Applications** | Surface fitting | Shape generation & interpolation |\n",
    "| **Generalization** | Fixed geometry | Variable anatomy |\n",
    "| **Shape Space** | Single point | Multi-dimensional space |\n",
    "\n",
    "### üß™ Next Steps & Advanced Explorations\n",
    "\n",
    "**Shape Interpolation** üåü\n",
    "```python\n",
    "# Blend between two different bones\n",
    "latent_A = lat_vecs(torch.tensor(0))  # Patient A\n",
    "latent_B = lat_vecs(torch.tensor(1))  # Patient B\n",
    "latent_hybrid = 0.7 * latent_A + 0.3 * latent_B  # 70% A, 30% B\n",
    "# Use latent_hybrid to generate a morphed bone shape!\n",
    "```\n",
    "\n",
    "**Shape Space Analysis** üìä\n",
    "- Compute distances between latent codes ‚Üí bone similarity analysis\n",
    "- Principal Component Analysis on latent vectors ‚Üí major shape variation modes\n",
    "- Clustering latent codes ‚Üí discover bone types/populations\n",
    "\n",
    "**Novel Shape Generation** üé≤\n",
    "- Sample random latent vectors ‚Üí generate entirely new bone variations\n",
    "- Conditional generation based on patient metadata (age, gender, etc.)\n",
    "- Shape completion: predict missing bone regions\n",
    "\n",
    "**Scaling to Full 3D** üåç\n",
    "- Extend to full 3D coordinates (x,y,z) instead of 2D slices\n",
    "- Higher resolution grids for detailed reconstructions\n",
    "- Multiple anatomical structures (femur, humerus, etc.)\n",
    "\n",
    "\n",
    "- **Virtual reality**: Haptic interaction with anatomical models\n",
    "\n",
    "### üí° Research Directions\n",
    "\n",
    "This tutorial demonstrates the foundations of **neural implicit shape modeling** - a rapidly advancing field with connections to:\n",
    "\n",
    "- **3D Deep Learning**: Point clouds, voxels, meshes\n",
    "- **Generative AI**: VAEs, GANs, diffusion models for 3D\n",
    "- **Medical AI**: Shape-based diagnosis and treatment planning\n",
    "- **Computer Vision**: 3D reconstruction from images\n",
    "\n",
    "### üéì Key Takeaways\n",
    "\n",
    "1. **Latent embeddings** enable single networks to represent multiple shapes\n",
    "2. **Registration and normalization** are critical for multi-shape learning\n",
    "3. **Regularization** promotes smooth, interpolatable shape spaces\n",
    "4. **Neural SDFs** provide continuous, resolution-independent shape representation\n",
    "5. **Generative modeling** opens doors to shape synthesis and analysis\n",
    "6. **Shape space arithmetic** enables semantic manipulation of 3D geometry\n",
    "\n",
    "\n",
    "**Happy generating!** üöÄ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NSM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
